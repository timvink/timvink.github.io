{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TimVink.nl","text":"<p>Welcome to Tim Vink.nl</p>"},{"location":"about/","title":"About","text":"<p>I\u2019m a data literate digital native.  I dream about a future where technology will help create an abundant future for mankind.  Technology is changing the world for the better.  My goal is to be part of the change and help shape our future. </p> <p>I have 11 years of experience building machine learning models, of which 6 years managing machine learning teams. Currently, I manage a ML Engineering team at Allianz Direct. Previously I have managed a ML team at ING Bank, worked as a data scientist in different countries, and worked as a consultant in the healthcare industry building simulations models.</p> <p>You can contact me at vinktim@gmail.com.</p>"},{"location":"about/#open-source-projects","title":"Open source projects","text":"<p>A selection open source projects I have built or have been heavily involved in designing and building.</p> Project Usage Contributors timvink/mkdocs-git-revision-date-localized-plugin timvink/mkdocs-git-authors-plugin-plugin timvink/mkdocs-table-reader-plugin timvink/mkdocs-print-site-plugin timvink/mkdocs-enumerate-headings-plugin timvink/mkdocs-charts-plugin ing-bank/probatus ing-bank/skorecard ing-bank/doing-cli"},{"location":"blog/","title":"Tim's blog","text":"Get notified of new posts Email Address *          /* real people should not fill this in and expect good things - do not remove this or risk form bot signups */"},{"location":"blog/google-cloud-functions/","title":"How to develop Python Google Cloud Functions","text":"<p>I've been using Google's Cloud Functions for a project recently. My use case was building a small webscraper that periodically downloads excel files from a list of websites, parses them and write the structured data into a BigQuery database. In this blogpost I'll show you why going serverless with cloud functions is so amazing and cost-effective. And I'll discuss some practical problems that have solutions you can't easily find in the documentation or stackoverflow. </p> <p>In this blog:</p> <ul> <li>The basics</li> <li>Integrate with Cloud Storage</li> <li>Debugging your Cloud Functions</li> <li>Testing Cloud Functions locally</li> <li>Troubleshooting</li> <li>Bonus: Static sites with serverless backend</li> <li>Conclusion</li> </ul>"},{"location":"blog/google-cloud-functions/#the-basics","title":"The basics","text":"<p>Cloud Functions are small pieces of code that execute in an event-driven manner. They do one small thing very efficiently in reaction to a trigger\u200a\u2014\u200ausually an HTTP request. The neat thing is you manage zero infrastructure and usually only pay for the execution of your function and a few seconds of compute. You may have heard of other Functions-as-a-Service offerings such as AWS Lambda. (source)</p> <p>This means you can write a python 3.7 function and deploy it without handling infrastructure, OS or networking. Google takes care of scaling, whether you call your function once a month or millions of times a day. You only pay per 100ms of functions running, which means using Cloud Functions is much cheaper than deploying on your own servers (see pricing), with a generous free tier as well.</p> <p>Let's try it out. Create a project on Cloud Console and setup billing (don't worry; all examples below are in free tier). Install the Google Cloud SDK (<code>brew cask install google-cloud-sdk</code> on macOS), login and setup some defaults:</p> <pre><code>gcloud auth login\ngcloud config set project &lt;PROJECT_ID&gt;\ngcloud config set compute/region europe-west1\ngcloud config set compute/zone europe-west1-b\n</code></pre> <p>Cloud Functions have their own independent environment, so we put them in their own folder (tip: use the function name). The folder should have a <code>main.py</code> file and optionally if you need extra packages a <code>requirements.txt</code> file. Here is a simple project structure:</p> <pre><code>tree myproject\n#&gt; myproject\n#&gt; \u251c\u2500\u2500 README.md\n#&gt; \u2514\u2500\u2500 functions\n#&gt;     \u2514\u2500\u2500 hello_world\n#&gt;         \u2514\u2500\u2500 main.py\n</code></pre> <p>Cloud functions are called either by events in your project cloud environment, or by certain triggers (docs). We'll focus on HTTP trigger functions. Google uses Flask in their python runtime to handle incoming requests. This means the input for our function should be Flask request object, and the output should be a valid Flask response:</p> <pre><code># main.py\ndef hello_world(request):\n  return 'Hello World!'\n</code></pre> <p>Deployment is a breeze:</p> <pre><code>cd functions/hello_world\ngcloud beta functions deploy hello_world \\\n  --runtime python37 \\\n  --trigger-http \\\n  --region europe-west1\n#&gt; Deploying function (may take a while - up to 2 minutes)...done.\n#&gt; availableMemoryMb: 256\n#&gt; entryPoint: hello_world\n#&gt; httpsTrigger:\n#&gt;   url: https://us-central1-&lt;YOUR PROJECT_ID&gt;.cloudfunctions.net/hello_world\n#&gt; ...\n</code></pre> <p>Visit the httpsTrigger URL to view the output in your browser, or use <code>gcloud functions call hello_world</code> in your terminal.</p>"},{"location":"blog/google-cloud-functions/#integrate-with-cloud-storage","title":"Integrate with Cloud Storage","text":"<p>As an example we'll demonstrate how to integrate with a Cloud Storage bucket and create a cloud function that downloads a file for us. I'll use <code>gsutil</code> to create a bucket and upload an image of the SpaceX starship test vehicle.  </p> <pre><code>gsutil mb -l europe-west1 gs://&lt;your_bucket_name&gt;\ngsutil cp ~/Downloads/starship.jpeg gs://&lt;your_bucket_name&gt;\n#&gt; / [1 files][  5.9 KiB/  5.9 KiB]\n#&gt; Operation completed over 1 objects/5.9 KiB.\n</code></pre> <p>We could just make this bucket public, or share a signed URL to download this specific file. But for practice, we'll write the cloud function that will let us download the file. To get the image from the bucket into the python cloud function environment, we could use <code>tempfile.gettempdir()</code> to download it to the <code>/tmp</code> directory, an in-memory mount of the cloud function (source). Instead, we'll use <code>io.BytesIO</code> to create an object in memory directly. We'll use the <code>flask.send_file()</code> to return the image as a file:</p> <pre><code># main.py\nfrom io import BytesIO\nfrom flask import Flask, request, send_file\nfrom google.cloud import storage\nstorage_client = storage.Client()\n\ndef download_file(request):\n    bucket = storage_client.get_bucket('&lt;your bucket name&gt;')\n    blob = bucket.get_blob('starship.jpeg')\n    file = BytesIO(blob.download_as_string())\n    return send_file(file,\n        attachment_filename = blob.name,\n        as_attachment=True,\n        mimetype='image/jpeg')\n</code></pre> <p>To interface with the bucket from python, I'm using the google-cloud-storage package. This is not installed in the python runtime so we need to add it to a <code>requirements.txt</code> file:</p> <pre><code># requirements.txt\ngoogle-cloud-storage\n</code></pre> <p>You file structure should now look like:</p> <pre><code>tree myproject\n#&gt; myproject\n#&gt; \u251c\u2500\u2500 README.md\n#&gt; \u2514\u2500\u2500 functions\n#&gt;     \u251c\u2500\u2500 download_file\n#&gt;     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.py\n#&gt;     \u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n#&gt;     \u2514\u2500\u2500 hello_world\n#&gt;         \u2514\u2500\u2500 main.py\n</code></pre> <p>And then deploy with:</p> <pre><code>cd functions/download_file\ngcloud beta functions deploy download_file \\\n  --runtime python37 \\\n  --trigger-http \\\n  --region europe-west1\n</code></pre> <p>Visit the URL to download the image!</p>"},{"location":"blog/google-cloud-functions/#debugging-your-cloud-functions","title":"Debugging your Cloud Functions","text":"<p>So far it's easy sailing. But what happens if you cloud functions starts returning <code>Error: could not handle the request</code> ?</p> <p>To find the mistake, you have some options. In your project's cloud console, go to cloud functions and click on your function.</p> <ul> <li>In the general tab you can see the latest errors.</li> <li>In the testing tab you can run your functions and see some logs.</li> <li>The View Logs button shows python logs of your function.</li> </ul> <p>This workflow becomes annoying very quickly: it can take up to 2 minutes to deploy a new cloud function. And it does not always pinpoint the problem. In my logs I had a <code>finished with status: 'connection error'</code> and a test run returned that <code>Error: cannot communicate with function.</code>, both of which did not help me find the error. But there's a better way!</p>"},{"location":"blog/google-cloud-functions/#testing-cloud-functions-locally","title":"Testing Cloud Functions locally","text":"<p>Google describes how to use <code>unittest</code> to mock Flask and test a HTTP-trigger python function. This is great for unit testing, but for development I preferred to write a simple Flask app so I could call my functions locally:</p> <pre><code># DevelopmentFlaskApp.py (in root of project)\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"serviceaccount.json\"\n\nfrom flask import Flask, request, render_template, send_from_directory\nfrom functions.download_file import main as f_download_file\n\napp = Flask(__name__)\n\n@app.route('/download_file')\ndef flask_download_file():\n    return f_download_file.download_file(request)\n\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>Notice I set an environment variable pointing to a JSON. Our <code>google-cloud-storage</code> package uses these credentials to authenticate. We want to use the same permissions as the cloud function would have at runtime:</p> <p>At runtime, Cloud Functions uses the service account PROJECT_ID@appspot.gserviceaccount.com, which has the Editor role on the project. You can change the roles of this service account to limit or extend the permissions for your running functions. (source)</p> <p>Let's download these credentials to the root of our project (and if you use git, don't forgot to update <code>.gitignore</code>):</p> <pre><code>gcloud iam service-accounts keys create serviceaccount.json \\\n  --iam-account &lt;PROJECT_ID&gt;@appspot.gserviceaccount.com\n</code></pre> <p>To run our flask app:</p> <pre><code>pip install flask\nexport FLASK_APP=DevelopmentFlaskApp.py\nexport FLASK_ENV=development\nflask run\n</code></pre> <p>You can then visit <code>localhost:5000/download_file</code> to see if you cloud function works, without having to wait 2 minutes to deploy! And with the credentials json already downloaded, you could also opt to develop some functionality in a notebook.</p>"},{"location":"blog/google-cloud-functions/#troubleshooting","title":"Troubleshooting","text":"<p>In practice, you can run into all sorts of problems. Some of them were hard to debug and fix, so sharing them here:</p>"},{"location":"blog/google-cloud-functions/#zombie-deployments","title":"Zombie deployments","text":"<p>I had a lot of trouble getting one of my cloud functions to work. It worked perfect locally. I spent a lot of time reading about permissions, but it turns out the function was not overwritten after deployment (!!). Luckily, I'm not alone in this problem (github issue). Even deleting my function wouldn't stop the URL from working. The solution for me was re-deploying using a different name.</p>"},{"location":"blog/google-cloud-functions/#setting-permissions","title":"Setting permissions","text":"<p>If you happen to have a permission problem, it's fairly easy to solve. In Google Cloud, a policy binding is where a member (user or serviceaccount) gets attached to a role (which contains 1 or more permissions). Remember, for cloud functions the member will be the app engine service account. Next, find an appropriate predefined role for a cloud product. Here's an example of adding a policy binding using <code>gcloud</code>:</p> <pre><code>gcloud projects add-iam-policy-binding \\\n        --member serviceAccount:&lt;PROJECT_ID&gt;@appspot.gserviceaccount.com \\\n        --role roles/storage.admin\n</code></pre> <p>Sometimes, <code>gcloud</code> will ask you to enable a certain API. Here's an example for BigQuery:</p> <pre><code>gcloud services enable bigquery-json.googleapis.com\n</code></pre>"},{"location":"blog/google-cloud-functions/#connection-reset-by-peer","title":"Connection reset by peer","text":"<p>You might find an <code>ConnectionResetError: [Errno 104] Connection reset by peer</code> error in your logs, and it's not helpful at all. In my case, it had to do with creating clients for storage buckets and bigquery. This SO post confirmed my suspicion, that in ~10% of the cases creating the connection throws a connection reset error. The solution is a simple retry with some random wait time:</p> <pre><code>from google.cloud import storage\nfrom google.cloud import bigquery\nfrom retrying import retry\n\n@retry(stop_max_attempt_number=3, wait_random_min=1000, wait_random_max=2000)\ndef get_google_client(type):\n    if type == 'storage':\n        return storage.Client()\n    if type == 'bigquery':\n        return bigquery.Client()\n\nstorage_client = get_google_client('storage')\n</code></pre>"},{"location":"blog/google-cloud-functions/#finished-with-status-timeout","title":"Finished with status 'timeout'","text":"<p>If you call your HTTP function and get the very generic <code>Error: could not handle the request</code>, dive into the logs. You might find a <code>Function execution took 60003 ms, finished with status: 'timeout'</code>. I had missed it from reading the documentation, but cloud functions are capped to at most 60 seconds execution time. You can increase the timeout to up to 9 minutes. Alternatively, you need to split up your function. In my case, I had to create a separate cloud function to download and process each file on a webpage.</p>"},{"location":"blog/google-cloud-functions/#error-with-status-500","title":"Error with status 500","text":"<p>Another error that took me some time to figure out. I was invocating many functions at the same time using <code>asyncio</code>, and got really vague status 500 errors. Cause: Google Cloud Functions have many different types of limits and quotas, and: A function returns an HTTP 500 error code when one of the resources is over quota and the function cannot execute. For me it was that I was not using global variables to reuse objects in future invocations (see the best practices). Another way to solve it could be to move to background functions listening to pub/sub events, or increasing the quotas for your project.</p>"},{"location":"blog/google-cloud-functions/#bonus-static-sites-with-serverless-backend","title":"Bonus: Static sites with serverless backend","text":"<p>In my case I was hosting a static website with app engine and using cloud functions on the backend. I wanted to test and develop the site locally as a Flask app. In order to change the URL locally, you can use some javascript:</p> <pre><code>&lt;script&gt;\njQuery(document).ready(function(){\n  var gcf_download_file = \"https://us-central1-PROJECT_ID.cloudfunctions.net/download_file\"\n  if (location.hostname === \"localhost\" || location.hostname === \"127.0.0.1\") {\n    var gcf_download_file = \"/download_file\"\n  }\n  $('#my-download-link').attr(\"href\", gcf_download_file);\n});\n&lt;/script&gt;\n</code></pre>"},{"location":"blog/google-cloud-functions/#conclusion","title":"Conclusion","text":"<p>Cloud functions are extremely flexible and offer myriad possibilities. And because each invocation of a function has it's own environment, you can easily scale your code horizontally. If you want to build something a bit more complex, I recommend looking using pub/sub and letting cloud functions process a message queue.</p> <p>Good luck!</p>"},{"location":"blog/closest-coordinates/","title":"Quickly finding closest coordinates using K-D-Trees","text":"<p>This post will demonstrate how to quickly find for a given coordinate (latitude and longitude), the closest coordinate out of a list of other coordinates using K-D-Trees and the Euclidean distance.</p>"},{"location":"blog/closest-coordinates/#toy-problem-finding-population-of-nearest-town-for-a-given-address","title":"Toy Problem: Finding population of nearest town for a given address","text":"<p>Let's say we have a list of addresses, and we want to know the population of the nearest town. This can be useful as a feature for a machine learning model. To do this we can:</p> <ul> <li>Use Google's Geocoding API to convert addresses to coordinates.</li> <li>Download a free geonames dataset containing place names, population and coordinates.</li> </ul> <p>I downloaded and unzipped the polish geonames dataset (<code>PL.zip</code>) and loaded with:</p> <pre><code>import pandas as pd\ngeonames = pd.read_table(\"PL.txt\", header = None, names = [\"geonameid\", \"name\", \"asciiname\", \"alternatenames\", \"latitude\", \"longitude\",\"feature_class\", \"feature_code\", \"country_code\", \"cc2\", \"admin1_code\", \"admin2_code\", \"admin3_code\", \"admin4_code\",\"population\", \"elevation\", \"dem\", \"timezone\", \"modification_date\"])\n</code></pre> <p>Now we simply need to determine the nearest coordinate for each address, and take the corresponding population. Let's say we have 10k address coordinates, and 60k place coordinates, and for each address we want to find the closest place population.</p>"},{"location":"blog/closest-coordinates/#attempt-1-great-circles","title":"Attempt 1: Great Circles","text":"<p>The first thing I tried was great circles: the shortest distance between any two points on a sphere.</p> <p></p> <p>The great circle distance \\(d\\) is also known as the haversine distance, given by:</p> \\[ {\\displaystyle d=2r\\arcsin \\left({\\sqrt {\\sin ^{2}\\left({\\frac {\\varphi _{2}-\\varphi _{1}}{2}}\\right)+\\cos(\\varphi _{1})\\cos(\\varphi _{2})\\sin ^{2}\\left({\\frac {\\lambda _{2}-\\lambda _{1}}{2}}\\right)}}\\right)} \\] <p>Where:</p> <ul> <li>r: is the radius of the Earth</li> <li>\u03c61, \u03c62: latitude of point 1 and latitude of point 2</li> <li>\u03bb1, \u03bb2: longitude of point 1 and longitude of point 2</li> </ul> <p>Calculating the great circle distance between two coordinates is easy using a python package:</p> <pre><code>from geopy.distance import great_circle\nnewport_ri = (41.49008, -71.312796)\ncleveland_oh = (41.499498, -81.695391)\nprint(f\"distance is {great_circle(newport_ri, cleveland_oh).km}\")\n#&gt; distance is 864.2144943393627\n</code></pre> <p>I quickly ran into some problems however. Looping over 60k coordinates took ~8 seconds. We have 10.000 addresses so this is just going to take too long. The time complexity of our approach is about <code>O(n)</code>. We can do better by getting a little bit smarter. Let's start by understanding what coordinates are first.</p>"},{"location":"blog/closest-coordinates/#intermezzo-coordinates","title":"Intermezzo: Coordinates","text":"<p>Coordinates are expressed as degrees; where latitude (\\(\\phi\\)) is the north-south angle that ranges from -90\u00b0 (south pole), 0\u00b0 (equator) to 90\u00b0 (north pole). Early navigators could determine their latitude by measuring where Polaris (the North Star) was.</p> <p>Longitude (\\(\\lambda\\)) is the east-west line, where it is agreed that 0 degrees is the meridian passing through Greenwich, England. The longitude ranges from -180\u00b0 west to +180\u00b0 east. Using coordinates and the radius of the earth, you can point to any place on earth. Determining the longitude is much harder however. Only in 1612 did Galileo Galilei devise a method using the exact time and the orbits of the moons of Jupiter. Finally in 1773 John Harrison invented the marine chronometer, a portable timepiece accurate enough to allow ships at sea to determine their longitude.</p> <p></p>"},{"location":"blog/closest-coordinates/#attempt-2-euclidean-distance","title":"Attempt 2: Euclidean distance","text":"<p>Now that we understand coordinates, what if we simplify a bit and calculate a straight line from coordinate A to coordinate B, in 3D space? This is called the euclidean distance and is both easy and fast to calculate.</p> <p>In order to use the Euclidean distance we\u2019ll need to convert the latitude and longitude coordinates to the Cartesian plane; e.g. a x, y and z coordinate. To do that, we'll need to make some assumptions:</p> <ul> <li>The earth is a perfect sphere (it's not!)</li> <li>Disregard elevation completely</li> <li>Approximate the radius of the earth as 6371 km.</li> </ul> <p>Sure, we'll lose some accuracy, but that's fine given our use case. Denoting latitude by \\(\\phi\\), the longitude by \\(\\lambda\\) and the Earth's radius by \\(R\\), the cartesian coordinates \\(\\vec{r}\\) are given by (source):</p> \\[ \\vec{r}=\\left(\\begin{array}{c}x\\\\y\\\\z\\end{array}\\right) = \\left(\\begin{array}{c} R\\cos\\theta\\cos\\phi \\\\ R\\cos\\theta\\sin\\phi \\\\ R\\sin\\theta \\end{array}\\right)\\;. \\] <p>Note these are trigonometric functions that use radians instead of degrees. To convert our latitude and longitude degrees to radius, we simply multiply by $ \\pi / 180 $. Now the Euclidean distance between two cartesian coordinates is given by:</p> <p>$ d(\\vec{r_1},\\vec{r_2})=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2}\\;. $$</p> <p>Implementation in python:</p> <pre><code>import math\n\ndef cartesian(latitude, longitude, elevation = 0):\n    # Convert to radians\n    latitude = latitude * (math.pi / 180)\n    longitude = longitude * (math.pi / 180)\n\n    R = 6371 # 6378137.0 + elevation  # relative to centre of the earth\n    X = R * math.cos(latitude) * math.cos(longitude)\n    Y = R * math.cos(latitude) * math.sin(longitude)\n    Z = R * math.sin(latitude)\n    return (X, Y, Z)\n\ncartesian(*list((41.49008, -71.312796)))\n#&gt; (1529.060676779426, -4520.739477097017, 4220.726125555107)\n</code></pre> <p>A test with <code>%timeit</code> shows that it took about 850 nanoseconds to search over 60k coordinates! For 10k addresses, we can be done in 8.5 ms only at the cost of some accuracy. But our algorithm still has the same complexity <code>O(n)</code>: we will still be checking 60k place coordinates for every address coordinates. So it's not very scalable and we can do even better.</p>"},{"location":"blog/closest-coordinates/#finally-k-dimensional-trees","title":"Finally: K-dimensional trees","text":"<p>We can use a k-dimensional tree to organize our points in our 3-dimensional space. This is a data structure from computer science that can help with searches over multidimensional keys. Scipy has KDtree implemented, and for searches it uses euclidean distance by default (p=2) ! Let's build our tree data structure and a function to search it:</p> <pre><code>from scipy import spatial\n\nplaces = []\nfor index, row in geonames.iterrows():\n    coordinates = [row['latitude'], row['longitude']]\n    cartesian_coord = cartesian(*coordinates)\n    places.append(cartesian_coord)\n\ntree = spatial.KDTree(places)\n\ndef find_population(lat, lon):\n    cartesian_coord = cartesian(lat, lon)\n    closest = tree.query([cartesian_coord], p = 2)\n    index = closest[1][0]\n    return {\n        'name' : geonames.name[index],\n        'latitude' : geonames.latitude[index],\n        'longitude' : geonames.longitude[index],\n        'population' : geonames.population[index],\n        'distance' : closest[0][0]\n    }\n</code></pre> <p>A benchmark showed that instead of 8 sec, a search now took ~1.5 ms, more than 5000x faster! Actually, searching this tree structure has <code>O(log(n))</code> complexity. Some manual checks suggested the lost accuracy was only very marginal. A visual confirmation of a random coordinate in Warsaw shows that the algorithm is working as intented:</p> <p></p>"},{"location":"blog/closest-coordinates/#conclusion","title":"Conclusion","text":"<p>Go for the easiest and fastest solution whenever possible. But if you need some more speed, think about which assumptions you can make to simplify your problem. But before you do anything, properly analyse your dataset and think about your problem. In our toy case, geonames has many places with population zero, which we also could have excluded.</p>"},{"location":"blog/central-limit-theorem/","title":"From Central Limit Theorem to Bayes's Theorem via Linear Regression","text":"<p>Take any statistics course and you'll have heard about the central limit theorem. And you might have read about Bayes' theorem offering a different, more probabilistic method. In this long post I'll show how they are related, explaining concepts such as linear regression along the way. I'll use math, history, code, examples and plots to show you why both theorems are still very relevant for modern data scientists. </p> <p>We'll cover:</p> <ul> <li>Central Limit Theorem</li> <li>CLT in A/B Testing</li> <li>Linear Regression</li> <li>Using CLT to calculate uncertainty</li> <li>Bayes' Theorem</li> <li>Bayesian Linear Regression</li> <li>Discussion</li> </ul>"},{"location":"blog/central-limit-theorem/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>It's hard to understate the importance of this theorem for modern science. It was only discovered in 1733 by the French Abraham de Moivre, then forgotten en rescued again by Pierre-Simon Laplace in 1812. But the real importance was only discerned in 1901 by Aleksandr Lyapunov. Now it is considered the unofficial sovereign of probability theory (source).</p> <p>The Central Limit Theorem (CLT) states that if your population size is big enough (at least more than 30), the means of \\(n\\) random samples of your population will be normally distributed regardless of the underlying distribution of your population. Let's skip the math and the jargon and just try this out in R:</p> <pre><code>library(tidyverse)\nlibrary(ggplot2)\nlibrary(gridExtra)\nset.seed(1337)\n\npopulation &lt;- rweibull(10e5, shape = 1.5, scale = 1)\n\ngenerate_sample_means &lt;- function(population, sample_size, n_samples) {\n  rerun(n_samples,\n        population %&gt;% sample(sample_size) %&gt;% mean) %&gt;%\n    unlist\n}\nsample_means &lt;- population %&gt;%\n  generate_sample_means(sample_size = 100, n_samples = 1000)\n\np1 &lt;- tibble(x = population) %&gt;%\n  ggplot(aes(x = x)) + geom_histogram(bins = 100) +\n  ggtitle(glue::glue(\"'True' population with mean {round(mean(population),4)}\"),\n          subtitle = glue::glue(\"100k samples from Weibull(\u03bb = 1, k = 1.5)\"))\np2 &lt;- tibble(x = sample_means) %&gt;%\n  ggplot(aes(x = x)) + geom_histogram(bins = 50) +\n  ggtitle(glue::glue(\"Mean of means {round(mean(sample_means), 4)}\"),\n          subtitle = glue::glue(\"Means of 1000 samples of size 100\"))\ngrid.arrange(p1, p2, nrow = 1)\n</code></pre> <p></p> <p>Here we assume we know the entire true population of 100.000 values that have a clearly skewed weibull distribution (left plot). If we sample 100 values, record the average, and repeat 1000 times, we can start to see that distribution of sampled means looks like a normal distribution (right plot), with the mean of those sampled means approaching the actual mean of our population. Still confused? Check out this interactive visualization of the central limit theorem.</p> <p>This is already quite cool, but it gets more interesting when you see what happens when you increase the size of your (1000) samples:</p> <pre><code>tibble(sample_size = c(50,100,500,1000)) %&gt;%\n  mutate(sample_means = map(sample_size, generate_sample_means, 1000)) %&gt;%\n  unnest %&gt;%\n  ggplot(aes(x = sample_means)) +\n  geom_histogram(bins = 100) +\n  coord_cartesian(xlim = c(.6, 1.2)) +\n  facet_wrap(~ sample_size, nrow = 1, scales = \"free_y\")\n</code></pre> <p></p> <p>So as you increase your sample size, your sampled means will be closer to the the true mean of our population (in this case 0.9042). And because the sampled means are normally distributed, you can actually estimate from a sample what the expected distance is between the sampled mean and the true population mean. This is called the estimated standard error (\\(SE\\)) and calculated as:</p> \\[ \\sigma_{\\overline{x}}=\\frac{s}{\\sqrt{N}} \\] \\[ s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\overline{x}\\right)^{2}} \\] <ul> <li>\\(\\sigma_{\\overline{x}}\\) = (estimated) standard error of mean</li> <li>\\(s\\) = sample standard deviation</li> <li>\\(N\\) = sample size</li> <li>\\(x_{i}\\) = sample element \\(i\\)</li> <li>\\(\\overline{x}\\) = sample mean</li> </ul> <p>So if your estimated standard error is 0.1, this means that on average you expect your sample mean to be off by 0.1 to the true population average. As you can see in both the plot and the formula: the bigger your sample size, the lower your standard error. You can use the CLT and this standard error do all sorts of cool things. Let's start with a classic use-case: A/B Testing.</p>"},{"location":"blog/central-limit-theorem/#clt-in-ab-testing","title":"CLT in A/B Testing","text":"<p>Let's say we have a landing page and we've measured that 1.0 % of our visitors click our 'buy' button (Scenario A). This is the mean of many visitors not clicking (zeros) en a couple that do click (ones). Now we build a new and improved landing page (our B scenario), and show this to 200 randomly selected visitors instead, and the click rate (conversion) is now 1.2%. But does that mean our new landing page is actually better, or we just got lucky?</p> <p>We can use the CLT to find out how likely it is to find a sample mean of 1.2% given our 'null hypothesis' of mean 1%. Remember that for normal distributions, 95% of the values are within two standard deviations (= standard errors) of the mean \\(\\mu\\):</p> \\[ \\mu \\pm 2 \\cdot \\operatorname{SE}\\left(\\mu\\right) \\] <p></p> <p>You might know this as the 95% confidence interval. We need to be more precise however: exactly how many standard deviations (aka standard errors) is our sample mean (1.2 %) away from our population mean (1 %)? This is called the z-score (a.k.a. standard score) and is given by:</p> \\[ z=(x-\\mu) / \\sigma \\] <ul> <li>\\(z\\) = z-score (standard score)</li> <li>\\(x\\) = Mean of experiment</li> <li>\\(\\mu\\) = Mean of population</li> <li>\\(\\sigma\\) = Standard error of population (or estimated SE from sample)</li> </ul> <p>As you can see from the formula and the graph below (from wikipedia), the z-score is a nice normalized metric to say how far away our sampled mean is from the population mean.</p> <p></p> <p>Let's calculate the z-score for our A/B test. In this case we know of the 'true' population (our A scenario), so we do not have to estimate the standard error \\(\\sigma\\) from a sample but can calculate it from the logs. So let's say we measured that 1% of visitor click buy, with a 0.15% standard deviation. So \\(\\mu\\) = 1% and \\(x\\) is 1.2%, so the z-score is (0.012 - 0.01) / 0.0015 = 1.333. This means we now know that our new landing page has an average conversion that is 1.333 standard deviations away from our normal landing page. But how probable is that? Again, because our means are normally distributed, we need to know how much of the distribution is on or past our z-score; i.e. we need to know the area under the curve paste the z-score; i.e. we need to know the probability \\(P\\) that a value from distribution \\(Z\\) is equal to more than our z-score \\(z\\). This probability is called the p-value, and can be seen in the graph below. (source)</p> <p></p> <p>It's hard to compute the p-value by hand because we need to integrate to find the area under the curve. We could look up pre-computed values (f.e. see this z-score table), but it's easiest to compute using R:</p> <pre><code># Calculate p-value from z-score\npnorm(abs(1.33333), lower.tail = F)\n#&gt; 0.09121177\n</code></pre> <p>So we now know that there is a probability of ~9% that we find a conversion rate of 1.2% given a measurement of 200 visits to our old landing page. In order words; there is a 9% chance that our new landing page did not improve conversion but we were just lucky with our sample. In fact, it could even be that new landing page is worse than the current! So when do you decide that your new landing page is better than the old one? Often a p-value of 0.05 (5%) or even 0.01 (1%) is used, but ultimately, it's up to you determine the threshold. Often it's said that a p-value below 0.05 means something is 'significant', but this threshold makes no sense and you should carefully think about it yourself.. can you afford to be wrong?</p> <p>So we've learned you can use the central limit theorem to do A/B testing. You simply measure or assume a mean for A and then do an experiment that gives you a sample B from which you can calculate its standard error. You assume B is not different from A (your null hypothesis), and will only accept that B is different (reject your null hypothesis) if the probability of measuring the mean you have for B is very low (p-value below f.e. 5%).</p> <p>Want to go deeper?</p> <ul> <li>Build some intuition around p-values using this short explanation with puppies.</li> <li>p-values are very common in the scientific community but there is a lot of critique. TLDR; it can happen that your null hypothesis is not true or off, which means your false positive rate can be completely off.</li> <li>You can also turn this math around and determine up front the experiment (sample) size you need to make certain conclusions. If you expect your new landing page to lead to 5% more conversion, and you want to be very very sure it is (1% chance you're wrong), you would need X visitors (see math details).</li> </ul> <p>Next, let's look at how the CLT is also used in another very common machine learning algorithm, linear regression.</p>"},{"location":"blog/central-limit-theorem/#linear-regression","title":"Linear regression","text":"<p>The earliest form of linear regression was published by Legendre in 1805 (source). Now it is often the first algorithm data scientists are taught when they learn about machine learning. This old algorithm is even present in our modern deep learning algorithms: a fully connected feedforward neural net with linear activation functions and several layers will in essence simply perform linear regression (source).</p> <p>In machine learning, we assume we can predict a vector \\(Y\\) with some feature matrix \\(X\\) plus some random errors \\(\\epsilon\\):</p> \\[ Y = f ( X ) + \\epsilon \\] <p>In (multivariate) linear regression, we assume \\(f\\) can be modelled with \\(n\\) independent features that can be added together (additive assumption) and an intercept \\(\\beta _ { 0 }\\):</p> \\[ Y = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + ... + \\beta _ { n } X _ { n } + \\epsilon \\] <p>Simple right, except how do you find the \\(\\beta\\) values? Well if you make several assumptions such as that the error \\(\\epsilon\\) has mean zero, the Gauss-Markov Theorem states that the so-called Best Linear Unbiased Estimator (BLUE) is Ordinary Least Squares. This means you can find the best values for \\(\\beta\\) if you minimize the sum of the squared differences between your predictions \\(\\hat{y}\\) and actuals \\(Y\\). This is called the residual sum of squares \\(RSS\\):</p> \\[ \\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\] <p>There are two basic methods to find the values for \\(\\beta\\) that minimise \\(RSS\\). The first is a closed form mathemical solution called the normal equation (proof) that is surprisingly simple:</p> \\[ \\widehat { \\beta } = \\left( X ^ { \\prime } X \\right) ^ { - 1 } X ^ { \\prime } y \\] <p>Neat, but as \\(X\\) gets bigger (more features and/or observations) it becomes computationally expensive. An alternative is using optimization algorithms, of which gradient descent based algorithms are very popular in machine learning. These types of algorithms are typically much faster and can also be used for other types of models where no closed form solutions exist.</p>"},{"location":"blog/central-limit-theorem/#toy-problem","title":"Toy problem","text":"<p>Let's try this out on the famous mtcars dataset, which holds various characteristics for 32 cars. Let's try to predict the miles per gallon (<code>mpg</code>) of a car given it's weight (<code>wt</code>), horsepower (<code>hp</code>), quarter mile acceleration time (<code>qsec</code>) and whether or not the transmission is manual (<code>am</code>).  We'll do this using the normal equation, using <code>t()</code> for transpose and <code>%*%</code> for matrix multiplication:</p> <pre><code>X &lt;- mtcars %&gt;%\n   select(wt, hp, qsec, am) %&gt;%\n   mutate(intercept = 1) %&gt;%\n   as.matrix\nY &lt;- as.matrix(mtcars$mpg)\n\nbeta &lt;- matlib::inv(t(X) %*% X) %*% (t(X) %*% Y)\ncustom_model &lt;- tibble(feature = c(\"wt\",\"hp\",\"qsec\",\"am\", \"intercept\"),\n  coefficients = as.numeric(beta))\ncustom_model\n#&gt;   A tibble: 5 x 2\n#&gt;   feature   coefficients\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 wt                 -3.24  \n#&gt; 2 hp                 -0.0174\n#&gt; 3 qsec                0.810\n#&gt; 4 am                  2.93  \n#&gt; 5 intercept          17.4   \n</code></pre> <p>Note the little trick where I've added the intercept as a feature with value 1. This way we can estimate all coefficients in one go.</p>"},{"location":"blog/central-limit-theorem/#using-clt-to-calculate-uncertainty","title":"Using CLT to calculate uncertainty","text":"<p>So we've calculated (estimated) our coefficients, but remember our sample size (number of rows) was only 32. How likely is it that these estimates are actually true? The central limit theorem can be used here: it holds for our multivariate situation where we are estimating multiple coefficients. The key assumption here is that our 32 cars are a sample from a much larger population of cars (which makes sense). So our estimated coefficients are part of a sampling distribution, and if we could average the estimates obtained over a huge number of data sets (of size 32), then the average of these estimates would be spot on.</p> <p>Our estimated variance \\(\\hat{\\sigma}\\) (standard error) is given by:</p> \\[ \\hat{\\sigma}^{2}=\\frac{1}{N-p-1} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\] <p>Note from the formula that the standard error will be smaller when the sample size \\(N\\) increases. So in R we can calculate:</p> <pre><code>y_hat &lt;- X %*% beta\n\nrss &lt;- sum((y - y_hat)^2)\ndegrees_of_freedom &lt;- (nrow(mtcars) - 4 - 1)\nRSE &lt;- sqrt(rss / degrees_of_freedom) # Residual standard error\nRSE\n#&gt; 2.435098\n</code></pre> <p>Using that we can now for a particular coefficient \\(j\\) calculate the \\(z\\)-score (from ESL p.48):</p> \\[ z_{j}=\\frac{\\hat{\\beta}_{j} - 0}{\\hat{\\sigma} \\sqrt{v}_{j}} \\] <p>where \\(v_{j}\\) is the \\(j\\)th diagonal element of \\(\\left(\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1}\\) (the intuition here is that the more variability we have in the explanatory variable, the more accurately we can estimate the unknown coefficient see SO post). Note I've added the minus zero in the formula for clarify; our null hypothesis is that the true coefficient is zero (the feature does not influence our target).</p> <p>Using the \\(z\\)-score, we can again calculate the \\(p\\)-values. Note that we want a two sided hypothesis test: our null hypothesis is that the coefficient is zero, but it could in fact be positive of negative.</p> <pre><code>matrixinv &lt;- matlib::inv(t(X) %*% X)\ndiagonals &lt;- diag(matrixinv)\nzscores &lt;- beta / (RSE * sqrt(diagonals))\ncustom_model &lt;- custom_model %&gt;%\n  mutate(z_score = as.numeric(zscores),\n         p_value = 2 * pnorm(abs(z_score), lower.tail = F))\ncustom_model\n#&gt; feature   coefficients z_score  p_value\n#&gt; &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 wt             -3.24     -3.64 0.000274\n#&gt; 2 hp             -0.0174   -1.23 0.219   \n#&gt; 3 qsec            0.810     1.85 0.0649  \n#&gt; 4 am              2.93      2.09 0.0363  \n#&gt; 5 intercept      17.4       1.87 0.0613\n</code></pre> <p>We now have uncertainty estimates, and can see that there is a 21.9% chance that we found the current coefficient for horsepower <code>hp</code> (-0.0174) while it was actually zero (no influence on miles per gallon at all). We could use that information and build a new, simpler model without the <code>hp</code> feature.</p> <p>We've built a linear regression model from scratch and calculated uncertainty using the central limit theorem. Cool! Now let's check our math with the base R implementation. In R it's actually a one-liner using <code>lm()</code>, which is much much faster, and actually uses both C and fortran under hood (here\u2019s a deepdive).</p> <pre><code>lm_model &lt;- lm(mpg ~ wt + hp + qsec + factor(am), data = mtcars)\nsummary(lm_model)\n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)   \n#&gt; (Intercept) 17.44019    9.31887   1.871  0.07215 .\n#&gt; wt          -3.23810    0.88990  -3.639  0.00114 **\n#&gt; hp          -0.01765    0.01415  -1.247  0.22309   \n#&gt; qsec         0.81060    0.43887   1.847  0.07573 .\n#&gt; factor(am)1  2.92550    1.39715   2.094  0.04579 *\n#&gt; ---\n#&gt; Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n\n#&gt; Residual standard error: 2.435 on 27 degrees of freedom\n#&gt; Multiple R-squared:  0.8579, Adjusted R-squared:  0.8368\n#&gt; F-statistic: 40.74 on 4 and 27 DF,  p-value: 0.00000000004589\n</code></pre> <p>Some observations:</p> <ul> <li>Our estimates are almost the same, but there are some small differences due to rounding errors (also note <code>tibble</code> does not show all digits when printing)</li> <li>Our \\(p\\)-values are roughly the same. This is because for linear regression the normal distribution is replaced by the \\(t\\)-distribution. This distribution asymptotically approaches normal as the sample size increases, and is almost similar after \\(N\\) &gt; 30 (our sample size \\(N\\) was 32).</li> </ul>"},{"location":"blog/central-limit-theorem/#downside-linear-regression","title":"Downside linear regression","text":"<p>We've seen that concepts from CLT are very important in linear regression (LR). Linear regression is awesome because it's simple, super fast to calculate and the results are easily interpretable by humans (maybe with the exception of \\(p\\)-values). But there are some downsides:</p> <ul> <li>LR assumes our observed \\(Y\\) values are actually sampled from a larger, true population of \\(Y\\). In our sample toy case with cars this could be true, but this is not always the case.</li> <li>Often building LR models involves an iterative process of removing non-significant features and adding new ones. You can do this in many different ways, going forward (adding features) or backward (removing them), but there is no best way. Two data scientists could end up with two different but valid models using the same data.</li> <li>LR returns point estimates of our coefficients, which by itself tells us nothing about the uncertainty in its accuracy.   </li> </ul> <p>We could in fact alleviate some of these concerns and build a model on our <code>mtcars</code> dataset while dropping CLT all together.</p>"},{"location":"blog/central-limit-theorem/#bayes-theorem","title":"Bayes' Theorem","text":"<p>This Theorem has some interesting history. It was actually independently discovered and popularized by Pierre-Simon Laplace, who also rediscovered the central limit theorem! The theorem has been invented multiple times, the earliest record being that of a reverend called Thomas Bayes somewhere in the 1740s. Bayes's Theorem has been used to crack the enigma code, estimate the mass of Jupiter, and discover that slightly more human baby boys are born than baby girls. Today it is widely used but it was quite controversial for a long time. If you want to read more on the history see the blogpost A History of Bayes' Theorem or read the book: The Theory that would not die.</p> <p>What is the big idea? Basically we formulate what we already know (called our initial belief, prior or event A), and then when we see new data (event B), we update what we know (believe) accordingly (now called our posterior). The famous Bayesian equation is thus:</p> \\[ P(A | B)=\\frac{P(B | A) P(A)}{P(B)} \\] <p>Here we are expressing 'events' occurring using conditional probability: the probability that an event will happen given that another event took place. So Bayes Theorem in words reads: \"the probability of A given that B have occurred is calculated as the unconditioned probability of A occurring multiplied by the probability of B occurring if A happened, divided by the unconditioned probability of B.\" (source)</p> <p>You can also put this another way:</p> <ul> <li>\\(P(B \\mid A)\\) is the probability of data given your hypothesis, and is called the likelihood.</li> <li>\\(P(A)\\) is the probability of your hypothesis, and is called the prior</li> <li>\\(P(B)\\) is the probability of the data under any hypothesis, and is called the normalizing constant</li> <li>\\(P(A \\mid B)\\) is the probability of your hypothesis after seeing the data, called the posterior</li> </ul> <p>The normalizing constant \\(P(B)\\) is not often something we're interested in, and if we drop it we can still maintain proportionality. Which is why Bayes' theorem also often reads:</p> \\[ P(A | B)\\propto P(B | A) P(A) \\] <p>or in words:</p> \\[ \\text { posterior } \\propto \\text { likelihood } \\times \\text { prior } \\]"},{"location":"blog/central-limit-theorem/#bayesian-linear-regression","title":"Bayesian linear regression","text":"<p>In order to do bayesian linear regression on our <code>mtcars</code> dataset we need to create a statistical model with a likelihood function. We can do that by rewriting our original linear regression model (defined earlier in this post) as a probabilistic model:</p> \\[ \\begin{aligned} y_{i} &amp; \\sim \\mathcal{N}\\left(\\mu_{i}, \\sigma\\right) \\\\ \\mu_{i} &amp;=\\alpha+\\beta X_{i} \\end{aligned} \\] <p>So the miles per gallon <code>mpg</code> of a single car \\(y_{i}\\) can be modelled using a normal distribution \\(\\mathcal{N}\\) parametrized by mean \\(u_{i}\\). which is a linear function of \\(X\\) parametrized by intercept \\(\\alpha\\), coefficients \\(\\beta\\) and standard deviation \\(\\sigma\\) (see this SO post). Remember we will have as many coefficients \\(\\beta\\) as we have features (columns) in our dataset \\(X\\).</p> <p>Now to find the optimal values of these parameters we will use maximum likelihood estimation. Interestingly this is the same as estimating these values using Ordinary Least Squares estimation (see notes Andrew Ng, cs229).</p> <p>So we now have a statistical model to find the \\(\\text { likelihood }\\) of our parameters: \\(p(y \\mid \\alpha,\\beta,\\sigma)\\). We are interested in the \\(\\text {posterior}\\) distribution of our parameters: \\(p(\\alpha,\\beta,\\sigma \\mid y )\\). We still need to define our \\(\\text { prior }\\): \\(p(\\alpha,\\beta,\\sigma)\\). Again, Bayes' formula:</p> \\[ \\text { posterior } \\propto \\text { likelihood } \\times \\text { prior }\\\\ p(\\alpha,\\beta,\\sigma \\mid y) \\propto p(y \\mid \\alpha,\\beta,\\sigma) p(\\alpha,\\beta,\\sigma) \\] <p>The priors also need to be defined as distributions. Here you need to carefully consider encode what you know as a distribution. As an example, for gas mileage I know that recent cars have a <code>mpg</code> between 10 and 40 (source). Our model probably won't be able to explain everything, so my prior for the intercept \\(\\alpha\\) will be a normal distribution with mean 25 and standard deviation 15: \\(N(25,15)\\). We need to set priors for every coefficient in \\(\\beta\\) and the standard deviation \\(\\sigma\\) of our probabilistic model as well.</p> <p>What do you do if you have no clue on how to set your priors? I don't like cars enough to be an expert. We can say that we have no clue by using very uninformative priors (also known as flat or diffuse priors). An example would be a very wide uniform distribution like \\(U(-10^{99},10^{99})\\).</p> <p>Note that when we use very uninformative priors the influence of the likelihood (from the data) will be large, because \\(\\text{prior} \\propto 1\\). In bayesian linear regression this means the maximum likelihood of our posterior distributions of the parameters will yield the same result as the ordinary least squares method we used earlier. However, when we use very informative priors (a distribution with a high spike in a small numeric range), we will need a lot of data in order to make significant changes to our estimated parameters. In other words, the prior starts to lose weight when we add more data.</p>"},{"location":"blog/central-limit-theorem/#calculating-the-posterior-distribution","title":"Calculating the posterior distribution","text":"<p>From the bayesian formula you can see that we are multiplying probability distributions \\(likelihood * prior\\). This calculation is easier if both distributions are from the same probability distribution family, otherwise \"there may be no analytical solution for the posterior distribution\" source. In our case we have a statistical model with a normally distributed likelihood, so we would be wise to define a conjugate prior: a prior that is also normally distributed. The prior and posterior are then called conjugate distributions, and the posterior can then be derived analyically.</p> <p>After calculating the posterior, we are left with the joint distributions of our parameters: \\(p(\\alpha, \\beta, \\sigma \\mid y)\\). In order to find the marginal distributions \\(p(\\alpha \\mid y)\\), \\(p(\\sigma \\mid y)\\) and each of \\(p(\\beta \\mid y)\\), we would have to solve a whole set of integrals. Instead, to find those marginal distributions sampling algorithms are used, where each parameter is sampled conditional on the other parameters. Most popular are Markov Chain Monte Carlo techniques such as Gibbs Sampling, Metropolis\u2013Hastings Sampling and more recently No U-Turns Sampling (NUTS).</p> <p>Luckily there are many packages for bayesian inference in R to help us. I recommend <code>brms</code> because it is well documented and plays well with other packages. Using python? Have a look at pymc3 or the new TensorFlow probability library.</p>"},{"location":"blog/central-limit-theorem/#applying-bayesian-linear-regression","title":"Applying bayesian linear regression","text":"<p>After all that theory, fitting a bayesian linear regression model in <code>brms</code> on our <code>mtcars</code> dataset is actually quite simple:</p> <pre><code>library(brms)\ncustom_mtcars &lt;- mtcars %&gt;% mutate(am = factor(am))\nfit &lt;- brm(mpg ~ wt + hp + qsec + am,\n           data = custom_mtcars, family = gaussian(), chains = 2)\nfixef(fit)\n#&gt;             Estimate  Est.Error         Q2.5       Q97.5\n#&gt; Intercept 17.5712388 9.78331475 -1.545673562 36.30109293\n#&gt; wt        -3.2376061 0.97190016 -5.230973286 -1.34308464\n#&gt; hp        -0.0176161 0.01517784 -0.046647580  0.01238611\n#&gt; qsec       0.8023787 0.46047930 -0.132152623  1.69762655\n#&gt; am1        2.9189454 1.50775195  0.002281527  6.04845493\n</code></pre> <p>Notice that the estimates are very very similar to our earlier OLS estimation. This is because <code>brms</code> uses \"an improper flat prior over the reals\" (see <code>vignette(\"brms_overview\")</code>) by default. Because we used very uninformative priors what we see is basically the estimate the maximum likelihood of a probability distribution, which is the same also the OLS estimation. We can see the distributions by plotting the samples from the posterior marginal densities. <code>brms</code> offers plotting functionality, but let's extract the samples and do it manually:</p> <pre><code># Get MCMC Samples for each parameter\nsamples &lt;- fit %&gt;%\n  brms::as.mcmc() %&gt;%\n  as.matrix() %&gt;%\n  as_tibble()\n</code></pre> <p>By default <code>brms</code> takes 2000 samples. We can then use Kernel Density Estimation (KDE) to create a smooth curve to visualize the density. The <code>ggplot</code> has KDE built in with the <code>geom_density()</code> geom. Let's visualize the distribution of the estimates for weight <code>wt</code>:</p> <pre><code># Visualize\nplot_distribution &lt;- function(data, feature_name) {\n  data %&gt;%\n    pull(feature_name) %&gt;%\n    tibble(feature = .) %&gt;%\n    ggplot(aes(x=feature))+\n    geom_density(color=\"darkblue\", fill=\"lightblue\") +\n    geom_point(aes(y = feature)) +\n    ggtitle(glue::glue(\"Posterior distribution of {feature_name}\"))\n}\nsamples %&gt;% plot_distribution(\"b_wt\")\n</code></pre> <p></p> <p>Here we can visually see that the most likely value of weight is ~ -3, but also that given the <code>mtcars</code> dataset, we are very confident the coefficient is between -6 and 0. This plot is made so often that it is built in: <code>brms::plot(fit)</code>.</p> <p>But we can do better. I don't know much about cars but I can be a bit more specific on the priors:</p> <pre><code>priors &lt;- c(\n  set_prior(\"normal(25, 15)\", class = \"Intercept\"),\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"wt\"),\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"hp\"),\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"qsec\"),\n  set_prior(\"normal(0, 10)\", class = \"b\", coef = \"am1\")\n)\n</code></pre> <pre><code>fit2 &lt;- brm(mpg ~ wt + hp + qsec + am,\n           data = custom_mtcars,\n           prior = priors,\n           family = gaussian(), chains = 2)\nfixef(fit2)\n#&gt;           Estimate  Est.Error        Q2.5       Q97.5\n#&gt; Intercept 17.40913139 9.96990040 -3.01219958 37.57619153\n#&gt; wt        -3.20417139 0.94568225 -4.98073192 -1.29575873\n#&gt; hp        -0.01801657 0.01461242 -0.04755709  0.01124505\n#&gt; qsec       0.80701586 0.46487332 -0.12740591  1.74774618\n#&gt; am1        2.97567564 1.51970719 -0.01908289  5.92563120\n</code></pre> <p>So we specified the priors as distributions, and <code>brms</code> multiplied them with our likelihood distribution in order to update our posterior distributions. We can visualize the shift of using our more informative priors by plotting the marginal distribution of <code>weight</code> again:</p> <pre><code>tibble(\n  \"flat prior\" = samples$b_wt,\n  \"N(0,10) prior\" = samples_fit2$b_wt\n) %&gt;%\n  tidyr::gather(prior, sample) %&gt;%\n  ggplot(aes(x=sample)) +\n  geom_density(aes(fill=prior), alpha = .3) +\n  ggtitle(\"Shift in posterior density of weight wt\")\n</code></pre> <p></p> <p>Not much of a shift, but you get the point ;) Do more research to define better priors!</p> <p>One of the upsides of having distributions instead of exact estimates for our coefficients is that we can build many visualizations showing our uncertainties in estimations and predictions. The <code>bmrs</code> package has a lot built in, and the tidybayes offers many more. As an example, here is a plot of how weight <code>wt</code> influence the miles per gallon <code>mpg</code>, including our 95% uncertainty interval:</p> <pre><code>plot(marginal_effects(fit2, effects = \"wt\"))\n</code></pre> <p></p> <p>Finally, it can be hard to define good priors. It is recommend to scale predictors and outcomes in order to make it easier to define priors. Selecting the feature set can also be challenging. In our non-bayesian LR we explained features selection can be done by adding or dropping non-significant features (features that are likely to be zero) using forward- or backward selection. In the bayesian world there are other techniques you can explore for this, such as bayesian model averaging.</p>"},{"location":"blog/central-limit-theorem/#discussion","title":"Discussion","text":"<p>We've come a long way and seen how the central limit theorem and Bayes' theorem relate. There has a been a lot of discussion historically about which is the better approach (frequentists vs bayesians). Interestingly we've seen both theorems were invented by the same man: mathematician Pierre LaPlace. Both theorems have their uses, and their up and downsides.</p> <p>Downsides Central Limit Theorem:</p> <ul> <li>Assumes your data is a sample from some much bigger population. This puts a lot of pressure on your dataset, in terms of size and how random your sample was. It assumes there are enough measurements to say something meaningful about \\(\\beta\\).</li> <li>We often use the 5% significance level to state that some hypothesis is 'true'. But this would also imply that by chance 5% of our scientific research is wrong!</li> <li>The \\(p\\)-values and significance tests rely on 'imaginary repetitions' (Harold Jeffreys)</li> </ul> <p>Bayes's Theorem also has downsides:</p> <ul> <li>Defining priors is very subjective as every scientist can define different priors for the same data. Critics say it is \"ignorance coined into science\" while CLT can be used for everything. However Frank Ramsey says as more data comes in scientists will tend to agree more (because the influence of priors will become smaller).</li> <li>You really have to put thought into your priors</li> <li>Calculating \\(\\text{likelihood} * \\text{prior}\\) is slow. MCMC Sampling requires a lot of computation.</li> <li>MCMC Sampling does not always converge.</li> </ul> <p>I believe bayesian models can really help for certain types of problems, especially because it offers you a way to encode your domain knowledge. In the end we still don't know for sure exactly how our features influence the mileage in cars, but we've learned techniques to communicate and visualize our estimates and their uncertainty.</p> <p>Hope you've learned something new. Let me know if you have any feedback!</p> <p>If you're interested: code for this blogpost</p>"},{"location":"blog/streamlit-threshold-app/","title":"Using streamlit to set the threshold for a classifier","text":"<p>In this post I will demonstrate how to use streamlit to build an app that can help interactively set the threshold for your machine learning classifier. See also the code and the live demo app.</p>"},{"location":"blog/streamlit-threshold-app/#introduction","title":"Introduction","text":"<p>Many machine learning classifiers can output probabilities instead of direct predictions (f.e. using sklearn's <code>.predict_proba()</code> method). To predict a class these probabilities can be cut-off at a certain threshold. This threshold often defaults at <code>&gt;=0.5</code>, but many business problems benefit from thoughtful setting of this threshold. This is especially true for unbalanced machine learning problems. Changing the threshold is inherently a tradeoff between precision and recall and should be done together with business stakeholders that understand the problem domain. And why not do that using an interactive app instead of a slide?</p>"},{"location":"blog/streamlit-threshold-app/#streamlit","title":"Streamlit","text":"<p>streamlit is an open source python library that makes it easy to build a custom web app. You can compare it to dash or R's shiny package. Dash is more fully featured and customizable, but for quick prototyping I find streamlit is much simpler and easier to learn.</p>"},{"location":"blog/streamlit-threshold-app/#modelling-problem","title":"Modelling problem","text":"<p>Let's say we have a straightforward binary classification problem. We generate a dataset <code>X</code> with 30k observations, 20 features and a class imbalance of 9:1. We'll use a stratified <code>train_test_split</code> with 80% train and 20% test. Next, we train a simple <code>RandomForestClassifier</code> model on the train set, using a 5-fold cross-validated grid search to tune the hyperparameters. If you want to see the code see model.py.</p> <p>To tune the threshold, we'll need to save the actuals and the predicted probabilities for both <code>train</code> and <code>test</code> datasets. With those 4 arrays, we can compute static performance metrics like roc_auc_score, but also metrics that depend on the threshold, like precision_score and recall_score. You can find the code in eval.py.</p>"},{"location":"blog/streamlit-threshold-app/#our-app","title":"Our app","text":"<p>In a new <code>app.py</code> file, we can add user interface elements like a title and slider with:</p> <pre><code>import streamlit as st\nst.title(\"Setting the threshold for our classifier\")\nthreshold = st.slider(\"Threshold\", min_value=0.00, max_value=1.0, step=0.01, value=0.5)\n</code></pre> <p>We also need to get the predicted probabilities and actuals from our model. Because we don't want to recalculate the entire model every time we change the model, streamlit offers the possibility to cache the results:</p> <pre><code>from model import get_predictions # custom model code\n\n@st.cache()\ndef cached_get_predictions():\n    return get_predictions()\n\ny_train, yhat_prob_train, y_test, yhat_prob_test = cached_get_predictions()\n</code></pre> <p>Next up is to calculate the metrics that depend on the threshold and display them as a table in the user interface:</p> <pre><code>from eval import get_metrics_df # custom code\n\nmetrics = get_metrics_df(\n    y_train, yhat_prob_train, y_test, yhat_prob_test, threshold=threshold\n)\nst.dataframe(metrics)\n</code></pre> <p>There are many other components I could add (see Streamlit API reference). I added a matplotlib plot to visualize the threshold setting. You can see the whole project github: timvink/demo_streamlit_threshold_classifier. Here's what it looks like:</p> <p></p> <p>use the demo app live</p>"},{"location":"blog/streamlit-threshold-app/#conclusion-further-reading","title":"Conclusion &amp; further reading","text":"<p>This was a basic example of setting a threshold. From here, you could consider extending the app to cover aspects like fairness (see for example Attacking discrimination with smarter machine learning).</p> <p>I find streamlit an easy to use and quick to learn library to add some quick interactivity to certain analysis. I still find it too involved for a one-off analysis, but for scenarios with some re-usability I find it well worth your time learning.</p> <p>Some good resources for further reading on streamlit:</p> <ul> <li>Streamlit video tutorial Crystal-clear and concise video tutorial by calmcode.io</li> <li>Streamlit 101: An in-depth introduction Great example use-case on NYC airbnb data</li> <li>Streamlit API reference Overview of all the streamlist elements</li> <li>Streamlit community components lists some quality custom components for streamlit</li> <li>awesome-streamlit list of streamlit resources</li> <li>github streamlit topic is a great way to discover more streamlit libraries</li> </ul>"},{"location":"blog/mkdocs-for-tech-doc/","title":"Using MkDocs for technical reporting","text":"<p>In this post I will explain why MkDocs is well suited for writing technical reports on machine learning models and introduce relevant parts of the MkDocs ecosystem.</p>"},{"location":"blog/mkdocs-for-tech-doc/#introduction","title":"Introduction","text":"<p>Machine learning might be a sexy field, writing documentation for models is not. towardsdatascience.com hardly has any blogposts on writing documentation. Spending time on writing good documentation with your models is often overlooked.</p> <p>In my work writing good technical documentation is an essential part of a machine learning project. The docs contain business context, list model performance metrics, document procedures around model updating and monitoring, explain important decisions, capture many crucially important considerations around model fairness, ethics, explainability, model limitations and much more. Our model documentation template contains guidelines that help ensure quality and consistency between projects, and promote best practices.</p> <p>Writing documentation cannot be made more fun, but it can be made easier. There a lot of different tools and packages out there, and it can take a long time to find a good setup to create a smooth documentation workflow. In this post I'll discuss some alternatives and introduce you to MkDocs and some of the most useful tools in the MkDocs ecosystem for writing technical documentation.</p>"},{"location":"blog/mkdocs-for-tech-doc/#mkdocs-vs-alternatives","title":"MkDocs vs alternatives","text":"<p>A good solution for writing documentation should be:</p> <ul> <li>Version controllable (flat files part of your git repo)</li> <li>Easy to convert to HTML and PDF</li> <li>Easy to learn  (also for less-technical project members)</li> <li>Easy to edit (also for less-technical project members)</li> <li>Able to insert content dynamically</li> <li>Python-based (easy to install)</li> <li>Really good looking! \ud83d\udc83</li> </ul> <p>MkDocs fits all these requirements, for but reference, there are some alternatives:</p> <ul> <li>There are several solutions that use nbconvert to convert jupyter notebooks to HTML or PDF. fastpages creates a blog site from a directory of notebooks, and the newly rewritten jupyter-book is a great solution for writing an entire book from notebook(s). The notebook file format however is JSON-based and contains not only markdown, but also the code and the output. That makes it hard to collaborate on a notebook in a team through version control (f.e. reviewing changes).</li> <li>Then there is Sphinx, a mature package initially for documenting python packages. It has a large eco-system of packages to extend sphinx, and can also support markdown via recommonmark. However I find it is harder to setup and learn if you're just looking to write simple documentation instead of documenting python objects.</li> </ul>"},{"location":"blog/mkdocs-for-tech-doc/#mkdocs","title":"MkDocs","text":"<p>MkDocs is a simple site generator that can create a website from a directory of markdown files. It's easy to extend through 1) themes, 2) plugins and 3) extensions to the markdown language. Because it uses simple markdown files, you can write docs in your IDE and use your normal git workflow. That keeps your code in sync with your docs, which definitely beats sending around lots of <code>report_final22.docx</code> files via email!</p> <p>Because MkDocs offers such a large eco-system, it's easy to miss some of the best functionalities. The rest of the blog will give you a quick tour:</p>"},{"location":"blog/mkdocs-for-tech-doc/#the-bare-basics","title":"The bare basics","text":"<p>In your project directory, run:</p> <pre><code>pip install mkdocs\nmkdocs new .\n</code></pre> <p>Which will create these files:</p> <pre><code>./\n\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 mkdocs.yml\n</code></pre> <p>Which you view using <code>mkdocs build</code> (creates a new <code>site/</code> directory with your website), or using <code>mkdocs serve</code> (starts a local webserver).</p> <p>The <code>mkdocs.yml</code> file is where you can customize and extend MkDocs. One setting I recommend changing is setting use_directory_urls to <code>false</code>. This ensures you can specify relative paths to images and your website navigation works using local files as well.</p> <pre><code># mkdocs.yml\nuse_directory_urls : False\n</code></pre>"},{"location":"blog/mkdocs-for-tech-doc/#adding-a-theme","title":"Adding a theme","text":"<p>There are several themes for mkdocs, but there is only one really killer theme for MkDocs: mkdocs-material. To enable it, <code>pip install mkdocs-material</code> and add these lines to your <code>mkdocs.yml</code>:</p> <pre><code># mkdocs.yml\ntheme:\n  name: material\n</code></pre> <p>The mkdocs-material theme documentation is very structured and offers many options for customizing the theme (including a dark mode!).</p>"},{"location":"blog/mkdocs-for-tech-doc/#adding-markdown-extensions","title":"Adding markdown extensions","text":"<p>The basic markdown syntax can be expanded to include other use cases. The mkdocs-material theme already has styling support for the extensions in the pymdown-extensions package and has examples and guides for many of them. Make sure to consider enabling these extensions when writing technical reports:</p> <ul> <li>Abbreviations: Which also allow for defining a glossary centrally</li> <li>Admonitions: (collapsible) content blocks</li> <li>Code highlighting: Add code blocks with highlighting</li> <li>Data tables: Well styled tables</li> <li>footnotes: Add footnotes for references</li> <li>content tabs: Separate content into tabs</li> <li>MathJax: Add formulas to your page</li> <li>emojis: \ud83d\ude0e</li> </ul> <p>For reference, there are many more packages that extend the markdown syntax and can be used with MkDocs.</p>"},{"location":"blog/mkdocs-for-tech-doc/#adding-plugins","title":"Adding plugins","text":"<p>MkDocs is python based, and allows you to write plugins that execute scripts during many different points in the build process (where markdown is converted to HTML). There are a lot of useful plugins. I'd like to highlight five which I wrote specifically to make writing reproducible technical documents easier:</p> <ul> <li>mkdocs-print-site-plugin: Let users save your entire site as a PDF through File &gt; Print &gt; Save as PDF.</li> <li>mkdocs-git-authors-plugin: Display authors (from git) at the bottom of each page.</li> <li>mkdocs-git-revision-date-localized-plugin: Adds a 'Last updated' date (from git) to bottom of each page.</li> <li>mkdocs-table-reader-plugin: Directly insert CSV's using a <code>{% raw %}{{ read_csv('table.csv') }}{% endraw %}</code> syntax.</li> <li>mkdocs-enumerate-headings-plugin: Enumerate all headings across your website in order.</li> </ul> <p>Some other very useful plugins to explore:</p> <ul> <li>mknotebooks: Add your jupyter notebooks directly to your MkDocs site.</li> <li>mkdocs-awesome-pages-plugin: Simplifies configuring page titles and their order</li> <li>mkdocs-pdf-export-plugin: Programmatically export to PDF</li> <li>mkdocs-bibtex: Citation management using bibtex</li> <li>mkdocs-minify-plugin: Minifies HTML and/or JS files prior to being written to disk (faster page loading)</li> </ul>"},{"location":"blog/mkdocs-for-tech-doc/#conclusion-further-reading","title":"Conclusion &amp; further reading","text":"<p>MkDocs enables writing elegant docs that live next to your source code, and is a great fit for writing technical reports in data science teams. With the theme, plugins and markdown extensions introduced, you should have a great place to get started. For more info, see:</p> <ul> <li>mkdocs.org</li> <li>mkdocs-material</li> <li>list of mkdocs all plugins</li> <li>calmcode.io intro to mkdocs</li> </ul>"},{"location":"blog/introducing-skorecard/","title":"Introducing Skorecard for building better logistic regression models","text":"<p><code>skorecard</code> is an open source python package that provides scikit-learn compatible tools for bucketing categorical and numerical features and building traditional credit risk acceptance models (scorecards) on top of them. These tools have applications outside of the context of scorecards and this blogpost will show you how to use them to potentially improve your own machine learning models.</p>"},{"location":"blog/introducing-skorecard/#bucketing-features-is-a-great-idea","title":"Bucketing features is a great idea","text":"<p>Bucketing is a common technique to discretize the values of a continuous or categorical variable into numeric bins. It is also known as discretisation, quantization, partitioning, binning, grouping or encoding. It's a powerful technique that can help reduce model complexity, speed up model training, and help increase understanding of non-linear dependencies between a variable and a given target. Bucketing and then one-hot encoding features can introduce non-linearity to linear models. Even modern algorithms use bucketing, for example LightGBM that buckets continuous feature values into discrete bins and uses them to construct feature histograms during training (paper). Bucketing is also widely used in credit risk modelling (where linear models are often used) as an essential tool to help differentiate between high-risk and low-risk clients.</p> <p>Bucketing is a great idea to improve the quality of your features, especially if you are limited to linear models. With 'the right features a linear model can become a beast', as argued in Staying competitive with linear models. A good example is the explainable boosting machine (EBM) algorithm developed by Microsoft. It uses a novel way to detect interaction terms to breathe new life into generalized additive models, with performance comparable to state-of-the-art techniques like XGBoost (paper).</p> <p>Bucketing can be done univariately and can be unsupervised or supervised, and done on categorical or numerical values. Some examples:</p> <ul> <li>Unsupervised &amp; numerical: <code>sklearn.preprocessing.KBinsDiscretizer</code> that bins a continuous feature into <code>k</code> bins using different strategies.</li> <li>Unsupervised &amp; categorical: <code>sklearn.preprocessing.OrdinalEncoder</code> that encodes each unique value of a categorical feature into an ordinal integer.</li> <li>Supervised &amp; numerical: <code>skorecard.bucketers.DecisionTreeBucketer</code> trains a DecisionTreeClassifier on a single feature and uses the tree output to determine the bucket boundaries</li> <li>Supervised &amp; categorical: <code>skorecard.bucketers.OrdinalCategoricalBucketer</code> will create an ordinal bucket number sorted by the mean of the target per unique value, where the most common values will have a lower bucket number.</li> </ul>"},{"location":"blog/introducing-skorecard/#bucketing-in-skorecard","title":"Bucketing in skorecard","text":"<p><code>skorecard</code> offers a range of so-called bucketers that support pandas dataframes. There is support for different strategies to treat missing values as well as the possibility to define 'specials': certain values you would like to ensure get put into a separate bucket. And there are tools to inspect the resulting bucketing. Here's a simple example:</p> <pre><code>import pandas as pd\nfrom skorecard.bucketers import EqualWidthBucketer\n\ndf = pd.DataFrame({'feature1': range(10) })\nbucketer = EqualWidthBucketer(n_bins=5)\ndf = bucketer.fit_transform(df)\nbucketer.bucket_table('feature1')\n#&gt;    bucket        label  Count  Count (%)\n#&gt; 0      -1      Missing    0.0        0.0\n#&gt; 1       0  (-inf, 1.8]    2.0       20.0\n#&gt; 2       1   (1.8, 3.6]    2.0       20.0\n#&gt; 3       2   (3.6, 5.4]    2.0       20.0\n#&gt; 4       3   (5.4, 7.2]    2.0       20.0\n#&gt; 5       4   (7.2, inf]    2.0       20.0\n</code></pre> <p>When using bucketing in an actual use case like credit decision modelling, there are often all kinds of constraints that you would like to impose, like the minimal relative size of each bucket or monotonicity in the target rate per bucket. It's also common to discuss and incorporate domain expert knowledge into the bucketing of features. This manual 'fine tuning' of the bucketing is an interplay between art and science. <code>skorecard</code> offers support for bucketing under constraints (see OptimalBucketer) as well as a novel <code>.fit_interactive()</code> method that starts a dash webapp in your notebook or browser to interactively explore and edit buckets. Expanding on the example above, we can also do:</p> <pre><code>bucketer.fit_interactive(X, y)\n</code></pre> <p></p>"},{"location":"blog/introducing-skorecard/#weight-of-evidence-encoding-can-improve-performance","title":"Weight of Evidence encoding can improve performance","text":"<p>After bucketing, Weight of Evidence (WoE) encoding can help to improve binary logistic regression models. WoE encoding is a supervised transformation replaces each bucket (category) in a feature with their corresponding weight of evidence. You can find the weight of evidence for a bucket by taking the natural logarithm of the ratio of goods (ones) over the ratio of bads (zeros). So, if a certain bucket has 60% ones and 40% bads, the WoE would be <code>ln(0.6/0.4)</code> = 0.405. More formally:</p> \\[ \\mathrm{WOE}=\\ln \\left(\\frac{\\% \\text { of non-events }}{\\% \\text { of events }}\\right) \\] <p>The higher the WoE, the higher the probability of observing <code>Y=1</code>. If the WoE for an observation is positive then the probability is above average, if it's negative, it is below average. Using WoE encoding after bucketing means you can build models that are robust to outliers and missing values (which are bucketed separately). The natural logarithm helps to compare transformed features by creating the same scale and it helps to build a strict linear (monotonic) relationship with log odds. When combined with a logistic regression this method has also been called the 'semi-naive bayes classifier' because of it's close ties with naive bayes (this blogpost for more details).</p> <p>A downside of WoE encoding is that you need buckets that have observations with both classes in the training data, otherwise you might encounter a zero division error. That's why <code>skorecard</code> bucketers have a <code>min_bin_size</code> parameter that defaults to 5%. In practice, it means you'll want to bucket features to max ~10-20 buckets. And if you're using cross validation, you need to make sure the bucketing and WoE encoder are part of the model pipeline.</p> <p>The WoE encoder is quite common and there are many python implementations available online. Instead of implementing another one, <code>skorecard</code> uses scikit-learn contrib's category_encoders.woe.WOEEncoder:</p> <pre><code>from skorecard import datasets\nfrom category_encoders.woe import WOEEncoder\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nwe = WOEEncoder(cols=list(X.columns))\nwe.fit_transform(X, y)\n</code></pre>"},{"location":"blog/introducing-skorecard/#skorecard-as-a-stronger-baseline-model","title":"skorecard as a stronger baseline model","text":"<p>A good data scientist will start with a stupid model because a good baseline model can tell you if your data has some signal and if your final shiny model actually provides a performance increase worth the additional complexity. You want to apply Occam's razor to machine learning: prefer the simpler model over the complex one if performance is more or less equal. But a big complex model that has spent a lot of compute on hyperparameter tuning will likely outperform a humble quick LogisticRegression model. Before declaring victory, applying some smart auto-bucketing and weight of evidence encoding might offer a more performant and fair comparison. And it's a one-liner in <code>skorecard</code>:</p> <pre><code>from skorecard import Skorecard\n\nbaseline_model = Skorecard()\n</code></pre> <p>I ran a benchmark using the UCI Adult, Breast Cancer, skorecard UCI creditcard, UCI heart disease and Kaggle Telco customer churn datasets. We compare a LogisticRegression with one hot encoding of categorical columns (lr-ohe), a variant with ordinal encoding of categoricals (lr-ordinal), a typical RandomForest with 100 trees  (rf-100), a XGBoost model (xgb) and of course a Skorecard model with default settings.</p> dataset_name model_name test_score_mean telco_churn lr_ordinal nan telco_churn xgb 0.825 telco_churn rf-100 0.824 telco_churn lr_ohe 0.809 telco_churn skorecard 0.764 heart skorecard 0.911 heart lr_ohe 0.895 heart lr_ordinal 0.895 heart rf-100 0.89 heart xgb 0.851 breast-cancer skorecard 0.996 breast-cancer lr_ohe 0.994 breast-cancer lr_ordinal 0.994 breast-cancer rf-100 0.992 breast-cancer xgb 0.992 adult xgb 0.927 adult lr_ohe 0.906 adult rf-100 0.903 adult skorecard 0.888 adult lr_ordinal 0.855 UCI-creditcard skorecard 0.627 UCI-creditcard lr_ohe 0.621 UCI-creditcard lr_ordinal 0.621 UCI-creditcard xgb 0.596 UCI-creditcard rf-100 0.588 <p>Table 1: Benchmarking skorecard with other baseline models. See also benchmark notebook if you want to reproduce the results.</p> <p>The table shows that <code>Skorecard</code> frequently outperforms <code>LogisticRegression</code> and even more complex algorithms (that haven't been tuned yet), but in line with the no free lunch theorem not always. So while <code>skorecard</code> is a great baseline model, you will always want to use a range of simple models to get a sense of the performance of your final model.</p>"},{"location":"blog/introducing-skorecard/#conclusion","title":"Conclusion","text":"<p>Bucketing features is a powerful technique that can help model performance and improve the interpretability of features. Bucketing in combination with weight of evidence encoding can make a logistic regression model much more powerful. <code>skorecard</code> has bucketing transformers you can use in your own modelling pipelines and a <code>Skorecard()</code> estimator that helps automate the bucketing+woe encoding technique and can be more competitive than LogisticRegression. <code>skorecard</code> is a strong approach that you could consider adding to your baseline models.</p> <p>In the <code>skorecard</code> documentation there are more interesting features to explore such as support for missing values (tutorial), special values for bucketing (tutorial), a two step bucketing approach (tutorial) and the interactive bucketing app (tutorial).</p>"},{"location":"blog/reproducible-reports-with-mkdocs/","title":"Reproducible Reports with MkDocs","text":"<p>In the post Using MkDocs for technical reporting I explained how MkDocs works and why it's a good choice for writing technical reports.</p> <p>In this post I'll explain how to work with different MkDocs plugins to make your documentation more reproducible. I find the topic exciting as the combination of these plugins is especially powerful. That's also why I wrote multiple MkDocs plugins and contributed to many more to make the workflow even smoother.</p> <p>We'll use the example of documenting a machine learning model because the building process is quite iterative. Because models evolve and update frequently up to date model documentation that can keep up is essential.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#what-artifacts-should-be-reproducible","title":"What artifacts should be reproducible","text":"<p>A model development proces might generate the following artifacts:</p> <ul> <li>Images: Binary images files, like <code>.png</code> files created by a plotting library.</li> <li>Variables: A list of one or more variables in <code>.yaml</code> files, such as for example training periods.</li> <li>Tables: Data structured in tabular manner, like <code>.csv</code> files</li> <li>Chart data: Data and chart specifications in <code>.json</code> files</li> <li>Standalone pages: <code>.HTML</code> files generated by tools like pandas-profiling</li> <li>Notebooks: Relevant <code>.ipynb</code> files with deep-dives that combine code + text + output</li> </ul> <p>We want to combine all of these artifacts into a coherent MkDocs documentation site.</p> <p>Schematically:</p> <p></p>"},{"location":"blog/reproducible-reports-with-mkdocs/#generating-artifacts-on-build","title":"Generating artifacts on build","text":"<p>Obviously you'll first need to write the python code that (re)generates all the artifacts you want to include in your model documentation. Here's a common project structure, where <code>scripts/</code> contains python code to generate artifacts that output to <code>docs/assets/</code>.</p> <pre><code>.                                \n\u251c\u2500\u2500 docs/                        # Project documentation\n|   |\u2500\u2500 assets/                  # Stores generated artifacts\n|   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 notebooks/                   # Project related Jupyter notebooks\n\u251c\u2500\u2500 src/                         # Project source code\n\u251c\u2500\u2500 scripts/                     # Stores scripts to generate artifacts\n\u2514\u2500\u2500 mkdocs.yml                   # MkDocs configuration\n</code></pre> <p>You could trigger building artifacts manually, by building a CLI (see click, typer or python-fire) or by using a makefile. But you could also trigger building artifacts on <code>mkdocs build</code> (or <code>mkdocs serve</code>) by using the mkdocs plugin mkdocs-simple-hooks. For example, if you have a function <code>generate()</code> in <code>scripts/artifacts.py</code>:</p> <pre><code># mkdocs.yml\nplugins:\n  - mkdocs-simple-hooks:\n      hooks:\n        on_pre_build: \"scripts.artifacts:generate\"\n</code></pre> <p>Because building artifacts can take quite some time for larger ML models, you can optionally disable the plugin while writing documentation by using environment variables:</p> <pre><code># mkdocs.yml\nplugins:\n  - mkdocs-simple-hooks:\n      enabled: !ENV [ENABLE_MKDOCS_SIMPLE_HOOKS, True]\n      hooks:\n        on_pre_build: \"scripts.artifacts:generate\"\n</code></pre> <pre><code>export ENABLE_MKDOCS_SIMPLE_HOOKS=false\nmkdocs serve\n</code></pre>"},{"location":"blog/reproducible-reports-with-mkdocs/#including-tables","title":"Including tables","text":"<p>Probably one of the most common artifacts are tables. I wrote mkdocs-table-reader-plugin to make it easy:</p> <pre><code># mkdocs.yml\nplugins:\n  - table-reader:\n      data_path: \"docs/assets/tables\"\n</code></pre> <p>And then in any markdown file you can insert a table using <code>{% raw %}{{ read_csv(\"tablename.csv\") }}{% endraw %}</code>.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#including-variables","title":"Including variables","text":"<p>Another very common artifacts are variables; basically singular named values. mkdocs-markdownextradata-plugin is an excellent plugin to help you get the job done:</p> <pre><code># mkdocs.yml\nplugins:\n  - markdownextradata:\n      data: docs/assets/variables\n</code></pre> <p>Add a file in <code>docs/assets/variables</code> like:</p> <pre><code># data.yml\nauc_train: 0.76\nauc_test: 0.74\n</code></pre> <p>And now you can insert variables in any markdown file using the syntax <code>{% raw %}{{ data.auc_train }}{% endraw %}</code>.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#including-charts","title":"Including charts","text":"<p>A variant on a table is a chart. Basically this is tabular data with some specification on how to visualize it. vegalite is an excellent solution, and I wrote mkdocs-charts-plugin to bring support to MkDocs.</p> <pre><code># mkdocs.yml\nplugins:\n  - charts\n\nextra_javascript:\n  - https://cdn.jsdelivr.net/npm/vega@5\n  - https://cdn.jsdelivr.net/npm/vega-lite@5\n  - https://cdn.jsdelivr.net/npm/vega-embed@6\n\nmarkdown_extensions:\n  - pymdownx.superfences:\n      custom_fences:\n        - name: vegalite\n          class: vegalite\n          format: !!python/name:mkdocs_charts_plugin.fences.fence_vegalite\n</code></pre> <p>Now you can insert chart specifications anywhere in a markdown file, and have it rendered as a chart.</p> <pre><code>```vegalite \n{\n  \"description\": \"A simple bar chart with embedded data.\",\n  \"data\": {\"url\" : \"assets/charts/data/basic_bar_chart.json\"},\n  \"mark\": {\"type\": \"bar\", \"tooltip\": true},\n  \"encoding\": {\n    \"x\": {\"field\": \"a\", \"type\": \"nominal\", \"axis\": {\"labelAngle\": 0}},\n    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n  }\n}\n```\n</code></pre> <p>Note that the data is located in <code>assets/charts/data/basic_bar_chart.json</code>, which means you can update it separately.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#including-notebooks","title":"Including notebooks","text":"<p>Especially when building machine learning models, there might be separate, fairly self-contained deep dives in notebooks you would like to include in your documentation. For example an experiment with different target definitions, or a review of fairness aspects. A notebook already contains code, text and output (tables and images).</p> <p>The mknotebooks plugin allows you to insert entire jupyter notebooks (<code>.ipynb</code> files) directly into your MkDocs site. For example:</p> <pre><code># mkdocs.yml\nnav:\n  - your_notebook.ipynb\n\nplugins:\n  - mknotebooks\n</code></pre> <p>It's not well documented, but you can optionally remove input cells (the code) from the notebooks. See this example.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#including-html","title":"Including HTML","text":"<p>There are some great machine learning tools out there that can generate stand-alone HTML reports. For example pandas-profiling for exploratory data analysis, or great expectations for data profiling.</p> <p>You can export those files to <code>docs/assets/html</code> and simply link to them from your MkDocs navigation: <pre><code># mkdocs.yml\nnav:\n  - index.md\n  - Some page: assets/html/page.html\n</code></pre></p> <p>If it's a small snippet of HTML, you can also use the snippets markdown extension to embed external files, see the example the mkdocs-material docs.</p>"},{"location":"blog/reproducible-reports-with-mkdocs/#wrapping-up","title":"Wrapping up","text":"<p>If you need even more flexibility in inserting content, you can use the mkdocs_macros_plugin to define python functions. Using our machine learning example, you could write a python function <code>get_model_auc()</code> that queries MLFlow and returns a score. You can then use it anywhere in your markdown files using <code>{% raw %}{{ get_model_auc() }}{% endraw %}</code>.</p> <p>You might also want to use mkdocs-git-authors-plugin to automatically add authors of each page by using information from git commits, or mkdocs-git-revision-date-localized-plugin to add the last edited date to each page.</p> <p>With a now fully reproducible MkDocs website, you can use mkdocs-print-site-plugin export to a PDF or a standalone HTML file (that you can share over HTML). </p>"},{"location":"blog/post-balanced-trees/","title":"Adjusting the bootstrap in Random Forest","text":"<p>The RandomForest algorithm was introduced by Breiman back in 2001 (paper). In 2022 it is still a commonly used algorithm by many data scientists. The only difference is that the current scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class (source).</p> <p>In practice many classification problems are imbalanced: one class occurs more often than the other. In 2004, three years after the original paper, Breiman authored the paper 'Using Random Forest to Learn Imbalanced Data' (Chen, Chao, Andy Liaw, and Leo Breiman). In it, he explains:</p> <p>In learning extremely imbalanced data, there is a significant probability that a bootstrap sample contains few or even none of the minority class, resulting in a tree with poor performance for predicting the minority class.</p> <p>You can fix this by adjusting the bootstrap method in the random forest algorithm. For example by using a stratified bootstrap: sample with replacement from each class, which keeps the class imbalance. A further improved called the balanced random forest is proposed by Breiman, which first bootstraps the minority class and then samples with replacement the same number of cases from the majority class.</p> <p>The paper states that balanced RF has 'performance superior to most of the existing techniques that we studied'. In this blogpost we'll replicate the results and run our own benchmarks, and see if and when a balanced RF would make sense to try in your own projects.</p>"},{"location":"blog/post-balanced-trees/#theory-first-bootstrapping-in-random-forests","title":"Theory first: Bootstrapping in Random Forests","text":"<p>In case you don't have the theory top of mind: Random Forests work by ensembling a collection (forest) of decision trees fitted on bootstrapped (random) subsets of the data.</p> <p>The real magic is in the bootstrapping. Rows (number of observations \\(n\\)) are sampled with replacement until you have another set of size \\(n\\). This means the same row can occurs multiple times in your sample. In fact, each bootstrap sample can be expected to contain \\(\\approx\\) 1/3 of observations (source). As an aside, Random Forests also take a random subsample without replacement of your columns (number of features \\(m\\)). This method is known as Random Patches (read more). Empirically a good value for classification problems is <code>max_features = sqrt(n_features)</code>.</p> Illustration of bootstrapping (source)"},{"location":"blog/post-balanced-trees/#experiment-setup","title":"Experiment setup","text":"<p>Let's generate some data to create an imbalanced binary classification with a 10% minority class:</p> <pre><code>from sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples = 10_000, n_features = 20, n_informative=15, n_redundant=2, n_classes=2, weights=[.9, .1], flip_y=0.05, random_state=42)\nX = pd.DataFrame(X)\n</code></pre> <p>Fitting a random forest is straight forward:</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2, random_state=42)\n\nrf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features='auto', random_state=42)\nrf.fit(X_train, y_train)\n</code></pre> <p>For evaluation of different model variants we'll compute AUC (using roc_auc_score) because it tells us how well the model is able to separate between classes (we are free to set the probability threshold later). Because in imbalanced classification problems we usually are interested in high prediction accuracy over the minority class, while maintaining reasonable accuracy for the majority class, weighted accuracy (aka <code>balanced_accuracy_score</code>, the average recall for each class) is also interesting. It does however require a probability threshold to be set so we will leave it out of the comparison for now.</p> <p>Another challenge with evaluating classifiers on imbalanced data is that we can get lucky. If we change the random seed for our <code>train_test_split()</code> we might get a very difference performance (despite using stratified sampling). To get an idea of the average performance of our model we can train and evaluate over multiple random seeds. Making sure your results are not dependent on a lucky random seed is something that is often overlooked (although I do recommend 42 ;)). You could do 20 repeats of 5-fold cross validation using <code>RepeatedStratifiedKFold</code> but instead I will just average the AUC metric over a fixed set of 100 random seeds used in <code>train_test_split()</code>.</p> <p>The most fair comparison between model variants would be to do a grid search with equal compute resources on each model. For the purposes of this blogpost, we'll use default random forest hyperparameters with the exception of using <code>max_depth</code> of <code>7</code> and <code>min_samples_leaf</code> of 150 to avoid overfitting. That gives us:</p> Model Train-AUC Test-AUC Sklearn RF 0.9280 0.8939"},{"location":"blog/post-balanced-trees/#step-1-our-own-randomforest-class","title":"Step 1: Our own RandomForest class","text":"<p>Before we can adapt the bootstrapping strategy, we need to implement a normal random forest to make sure we're not making any mistakes. We can re-use scikit-learn's <code>DecisionTreeClassifer</code> and overwrite the bootstrapping of the rows. I've highlighted parts relevant parts of my <code>CustomRandomForestClassifier</code> class, full code can be found at github.com/timvink/experiment-balanced-bootstrapping.</p> <p>The <code>.fit()</code> method bootstraps a sample of our data and fits a decision tree model and repeats this <code>n_estimator</code> times:</p> <pre><code>class CustomRandomForestClassifier():\n    \"\"\"\n    Custom implementation of RF where we can change the bootstrapping method.\n    \"\"\"\n    # ...      \n    def fit(self, X: pd.DataFrame, y: np.array):\n\n        if not isinstance(X, pd.DataFrame):\n            X = pd.DataFrame(X)\n        assert isinstance(y, np.ndarray)\n\n        if self.random_state is not None:\n            np.random.seed(self.random_state)\n\n        trees = list()\n        for i in range(self.n_estimators):\n\n            # Get our bootstrapped data\n            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)\n\n            # Fit a decision tree\n            tree = DecisionTreeClassifier(\n              max_depth = self.max_depth,\n              min_samples_leaf = self.min_samples_leaf,\n              max_features = self.max_features,\n              random_state = self.random_state+i\n            )\n            tree.fit(X_bootstrap, y_bootstrap)\n            trees.append(tree)\n\n        self.trees = trees\n        return self\n</code></pre> <p>The bootstrap itself is done with <code>numpy.randint</code>:</p> <pre><code>    # ...\n    def _bootstrap_sample(self, X: pd.DataFrame, y: np.ndarray) -&gt; Tuple[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Returns bootstrapped indices of X\n        (same number of rows, sampled with replacement)\n\n        Args:\n          X: pandas dataframe\n\n        Return:\n          nd.array with indices\n        \"\"\"\n        n_samples = X.shape[0]\n        indices = np.random.randint(low=0, high=n_samples, size=n_samples)\n        X_bootstrap = X.iloc[indices]\n        y_bootstrap = y[indices]\n        return X_bootstrap, y_bootstrap\n</code></pre> <p>And prediction is done with 'soft' voting (averaging all probabilities):</p> <pre><code>    # ...\n    def predict_proba(self, X: pd.DataFrame):\n        \"\"\"\n        Here we use a 'soft' voting ensemble\n        average all probabilities\n\n        See https://github.com/scikit-learn/scikit-learn/blob/1495f69242646d239d89a5713982946b8ffcf9d9/sklearn/ensemble/voting.py#L320\n        \"\"\"\n        probas = [clf.predict_proba(X) for clf in self.trees]\n        probas = np.asarray(probas)\n        avg = np.average(probas, axis=0)\n        return avg\n</code></pre> <p>Then we can verify the implementation is correct:</p> Model Train-AUC Test-AUC Delta Custom RF 0.8815 0.8459 0.0356 sklearn RF 0.8682 0.8375 0.0307 <p>Scores are similar but differ, which is likely due to a minor difference in implementation. We'll take it as is and use the custom RF as the baseline to start experimenting with different bootstrap techniques.</p>"},{"location":"blog/post-balanced-trees/#stratified-bootstrapping","title":"Stratified bootstrapping","text":"<p>Stratified Sampling means we make sure each bootstrap has the same percentage of samples of each target class as the complete set. Basically it's two bootstraps, one on the minority class and one on the majority class. Implementation is easy with sklearn's <code>resample()</code>:</p> <pre><code>from sklearn.utils import resample\n\nclass StratifiedRandomForest(CustomRandomForestClassifier):    \n    def _bootstrap_sample(self, X: pd.DataFrame, y: np.ndarray) -&gt; Tuple[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Stratified bootstrap sample.\n\n        This means the class ratio should be the same in the bootstrap.\n        \"\"\"\n        X_bootstrap, y_bootstrap = resample(X, y, stratify = y)\n        return X_bootstrap, y_bootstrap\n</code></pre> <p>Performance degrades ever so slightly, but is still very similar to the base class. This might be more effective if we did not already stratify our train/test split as well.</p> Model Train-AUC Test-AUC Delta Custom RF 0.8815 0.8459 0.0356 Stratified RF 0.8813 0.8450 0.0363"},{"location":"blog/post-balanced-trees/#balanced-bootstrapping","title":"Balanced bootstrapping","text":"<p>Breiman's balanced random forest \"first bootstraps the minority class and then samples with replacement the same number of cases from the majority class.\". In code:</p> <pre><code>class BalancedRandomForest(CustomRandomForestClassifier):\n    def _bootstrap_sample(self, X: pd.DataFrame, y: np.ndarray) -&gt; Tuple[pd.DataFrame, np.ndarray]:\n        \"\"\"\n        Balanced bootstrap. Implementation of Breiman's BalancedRandomForest.\n\n        We create a dataset with an articial equal class distribution:\n        first bootstraps the minority class and then samples with replacement the same number of cases from the majority class.\n        \"\"\"\n        # Find majority class, 0 or 1\n        counts = np.bincount(y)\n        minority_class = np.argmin(counts)\n        n_minority = counts[minority_class]\n\n        # bootstrap minority class\n        indices_minority = np.random.choice(\n            np.where(y == minority_class)[0],\n            size = n_minority,\n            replace = True)\n\n        # bootstrap majority class with minority size\n        indices_majority = np.random.choice(\n            np.where(y != minority_class)[0],\n            size = n_minority,\n            replace = True)\n\n        indices = np.hstack([indices_majority, indices_minority])\n        np.random.shuffle(indices) # in-place\n\n        X_bootstrap = X.iloc[indices]\n        y_bootstrap = y[indices]\n        return X_bootstrap, y_bootstrap\n</code></pre> <p>The results:</p> Model Train-AUC Test-AUC Delta Custom RF 0.8815 0.8459 0.0356 Stratified RF 0.8813 0.8450 0.0363 Balanced RF 0.8391 0.8171 0.0220 <p>The <code>BalancedRandomForest</code> algorithm actually performs significantly worse. But because each tree sees less observations (2x size of minority class instead of a bootstrap of all observations), we might want to change the hyperparameters to regularize less (f.e. by increasing <code>max_depth</code> or decreasing <code>min_sample_leaf</code>) to do the algorithm more justice. We should also note that in the current comparison the balanced RF is overfitting a lot less (difference between train and test is smaller).</p>"},{"location":"blog/post-balanced-trees/#over-under-sampling","title":"Over Under sampling","text":"<p>While we're at it, we also create a <code>OverUnderRandomForest</code> where we sample with replacement from the minority class until we have 50% of \\(n\\) (oversampling), and then sample with replacement from the majority class (undersampling).</p> <p>The results:</p> Model Train-AUC Test-AUC Delta OverUnder RF 0.9023 0.8574 0.0449 Custom RF 0.8815 0.8459 0.0356 Stratified RF 0.8813 0.8450 0.0363 Balanced RF 0.8391 0.8171 0.0220 <p>Interestingly, our <code>OverUnderRandomForest</code> now has the best test score, but also seems to overfit more (as each decision tree sees more unique observations).</p>"},{"location":"blog/post-balanced-trees/#balancing-class-weight","title":"Balancing class weight","text":"<p>Another 'trick' is to penalize mistakes on the minority class by an amount proportional to how under-represented it is. This is actually available as the <code>class_weight='balanced'</code> parameter in some sklearn algorithms and some packages like xgboost, and we'll throw it in there just for comparison.</p> <pre><code>rf = RandomForestClassifier(**rf_params, class_weight='balanced')\n</code></pre> <p>For this particular problem it does not seem to help much though:</p> Model Train-AUC Test-AUC Delta OverUnder RF 0.9023 0.8574 0.0449 Custom RF 0.8815 0.8459 0.0356 class weight balanced RF 0.8832 0.8452 0.0380 Stratified RF 0.8813 0.8450 0.0363 Balanced RF 0.8391 0.8171 0.0220"},{"location":"blog/post-balanced-trees/#other-tactics","title":"Other tactics","text":"<p>Simple undersampling majority or oversampling minority on train/test split is already a very performant tactic. Undersampling majority for random forests might have a slight advantage (Chen 2004). Another paper finds oversampling minority till balance the best strategy, at least for convolution neural networks (Buda 2017). It makes sense because undersampling means you throw away information, but you do need to have to computational resources to deal with the additional observations from oversampling.</p> <p>Another common strategy is to adjust the probability threshold for a classifier after training it, which you can visualize using yellowbrick.classifier.DiscriminationThreshold() and find interactively using streamlit, or programmatically using scikit-lego's Thresholder. You could even optimize your classifiers to focus on certain areas by considering partial AUC as a metric, which is supported in scikit-learn's <code>roc_auc_score(max_fpr=...)</code>.</p>"},{"location":"blog/post-balanced-trees/#what-about-other-imbalance-ratios","title":"What about other imbalance ratios?","text":"<p>If you look at the results when varying the imbalance ratios the scores the differences are marginal. <code>OverUnderSampling</code> does seems to have a slight edge, but I'm sure if you spend a bit of time on hyperparameter tuning of each variant the differences would be even smaller.</p> <p></p>"},{"location":"blog/post-balanced-trees/#conclusion","title":"Conclusion","text":"<p>Experimenting with bootstrap sampling is definitely interesting, and can give you a slight edge, but your time is better spent just understanding your data and the modelling problem rather than exploring these slight model variants. There is no silver bullet, and I will keep using the 'vanilla' random forest implementation as it's a great and fast baseline to use before applying more complex models like gradient boosted models.</p> <p>If you want to try a balanced RandomForest, you can use the BalancedRandomForest implementation from imbalanced-learn, which also offers a lot of other algorithms and sampling techniques.</p>"},{"location":"blog/is-xgboost-all-we-need/","title":"Is XGBoost really all we need?","text":"<p>If you have experience building machine learning models on tabular data you will have experienced that gradient boosting based algorithms like catboost, lightgbm and xgboost are almost always superior.</p> <p>It's not for nothing Bojan Tunguz (a quadruple kaggle grandmaster employed by Nvidia) states:</p> <p>XGBoost Is All You NeedDeep Neural Networks and Tabular Data: A Surveyhttps://t.co/Z2KsHP3fvp pic.twitter.com/uh5NLS1fVP</p>\u2014 Bojan Tunguz (@tunguz) March 30, 2022 <p>... but aren't we all fooling ourselves?</p>"},{"location":"blog/is-xgboost-all-we-need/#the-illusion-of-progress","title":"The illusion of progress","text":"<p>In his 2006 paper Classifier Technology and the Illusion of Progress David J. Hand argues that the 'apparent superiority of more sophisticated methods may be something of an illusion' and that 'simple methods typically yield performance almost as good as more sophisticated methods' and the difference in performance 'may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm'.</p> <p>Let's dive into his main arguments:</p>"},{"location":"blog/is-xgboost-all-we-need/#1-law-of-marginal-improvements","title":"1. Law of marginal improvements","text":"<p>The extra performance achieved by more sophisticated algorithms beyond those from simple methods, is small. </p> <p>There is a law of diminishing returns: Simple models will learn the basic, most apparent data structures that lead to greater improvements in predictive performance, while newer approaches learn the more complicated structures.</p> <p>The same goes for the number of features. For regression: If a set of features are all somewhat correlated with the target, there will be mutual correlation between them. We usually select those features with the highest target correlation first (for example with MRMR). Hand shows that (for regression) even with a low mutual correlation a small number of predictors will already explain most of the variance in the target. For classification a similar argument can be made: each feature added has a smaller maximum reduction in misclassification rate.</p> <p>So the proportion of gains attributable to the early steps is big: both in data structures learned by simple models, and with the first couple most promising features.</p>"},{"location":"blog/is-xgboost-all-we-need/#2-simple-classifiers-are-very-effective","title":"2. Simple classifiers are very effective","text":"<p>\"In many problems the bayes error rate is high, meaning that no decision surface can separate the distributions of such problems very well.\" But it is \"common to find that the centroids of the predictor variable distributions of the classes are different, so that a simple linear surface can do surprisingly well as an estimate of the true decision surface.\".</p> <p>To prove this, Hand not only refers to other studies, but also does his own. He takes 10 different datasets and defines the achieve performance in predictive accuracy for each as the difference between the best known model (in 2006 of course) and a majority-class vote baseline model. He then shows that simple classifiers are able to reach 85-95% of the achievable improvement for those datasets, with most over 90%.</p> <p>Another argument used for this is the \"flat maximum effect\" for linear models. When the correlations between features are high, often most of the gains can be made by simply assigning equal weights to each feature, without spending any time on optimizing the weights. In order words: quite large deviations from the optimal set of weights would yield performance not much worse than the optimal weights. This is because the simple average would be highly correlated with any other weighted sum: the choice of weight makes little difference to the scores.</p>"},{"location":"blog/is-xgboost-all-we-need/#3-population-bias","title":"3. Population bias","text":"<p>Hand calls it \"design sample selection\" and basically argues it's unlikely our assumption holds that our train splits are representative of the distribution we will use our model on. </p> <p>If there's a time component, the future data our model will use in production is unlikely to have the same distribution as our historic training data (f.e. predicting loan defaults of new customers using previous customers as training data). Things change in the future, and if you want your model to keep working, don't make it fit the training data too well. Hand quotes Eric Hoffer saying \"In times of change, learners inherit the Earth, while the learned find them-selves beautifully equipped to deal with a world that no longer exists.\". </p> <p>If there's a sampling component, it's unlikely the population is the same. For example medical screening of new cases using a sample of previous patients: it's hard to find a sample where you are confident it sufficiently represents the entire population. Another example is in credit risk, where you are using training data of accepted client loans to predict whether new customer will default, but you will never know what historic rejected clients would have done (a problem which even has it's own name, reject inference.)</p> <p>So again, simple models pick up the bigger patterns which generalize better, while complex models are more susceptible to population bias and 'effort spent on overrefining the model is probably wasted effort'. Even computationally expensive sampling techniques such as repeated, grouped K-fold cross validation (see Sebastian Raschka's 2018 paper Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning) cannot not fix population bias.</p>"},{"location":"blog/is-xgboost-all-we-need/#4-problem-bias","title":"4. Problem bias","text":"<p>We often just assume there are no errors in the labels (<code>y</code>). Errors in the label means the variance of the estimated decision surface will be greater: it's better to stick to simpler models because the decision surface is flatter. </p> <p>The 2021 paper by Northcut et al confirm Hand and actually show errors in tests sets are numerous and widespread: on average ~3.4% across benchmark ML test sets! (see labelerrors.com). They conclude:</p> <p>\"Surprisingly, we find lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data.\" </p> <p>Label definition is another example of problem bias. The rules to create labels can be somewhat arbitrary, such as 3 months missed payments to define a 'default', grades above an 8 to define 'gifted' students and business rules to define 'good' and 'bad' customers. If the definitions are somewhat arbitrary, they can change over time, so there is no point in creating an overrefined model. </p> <p>When building models, often the metric used to select models is different than the metric to optimize the model (loss function), and both are different from the performance metric that actually matters in production. This simplification is often used (cost-based metrics are not common) but we should accept it introduces a bias. </p>"},{"location":"blog/is-xgboost-all-we-need/#5-flawed-benchmarks","title":"5. Flawed benchmarks","text":"<p>Finally, Hand argues that all algorithms require fine-tuning, which requires expertise in the methods. Obviously inventors know their own method best, so their papers and benchmarks are biased. Furthermore, average performance on standardized datasets like UCI are not representative for real-world machine learning problems. Different algorithms also require different pre-processing to perform best. And finally, as for the reasons explained above, \"small differences in error rate ... may vanish when problem uncertainties are taken into account\".</p> <p>Another argument from the paper is that method 1 may be 3x better than method 2, but if the error rate is already (very) low this might correspond to only a small proportion of the new data points. In order words, making 5 or 15 mistakes doesn't really matter when you're right 10.000 times. </p>"},{"location":"blog/is-xgboost-all-we-need/#counter-arguments","title":"Counter arguments","text":"<p>Hand offers many convincing arguments, but does that mean we should always go for simple models? Is XGBoost not all we need?</p> <p>Things have changed a lot since 2006. </p> <p>Let's start with the law of marginal improvements (1). This is true of course, but anno 2022 our problems usually have a lot more data available. I would argue that for many real-life problems with large datasets GBM algorithms (like XGBoost) are often on the sweet spot between model complexity and the ability to generalize. Let's consider even more complex algorithms. The 2022 paper by Borisov et al. has an overview of the state-of-the-art deep learning methods for tabular data. They found that \"In general, however, our results are consistent with the inferior performance of deep learning techniques in comparison to approaches based on decision tree ensembles (such as gradient boosting decision trees) on tabular data that was observed in various Kaggle competitions\".</p> <p>Then there's the flawed benchmarks (5) argument. Creating different model and data preprocessing pipelines is now much easier because of better ML tools. Consider that scikit-learn was created in 2007 and had it's first beta release in 2010 (wiki). Regarding bias introduced by hyperparameter tuning: We now have much more compute power for hyperparameter tuning and better tools to run them: We can easily create fair benchmarks by using more compute. In the 2022 paper Why do tree-based models still outperform deep learning on tabular data? Grinszatjn et al build a standardized benchmark on many different datasets and give each algorithm a fair 20.000 hours of hyperparameter tuning. Not only is their code open source (Github only in started in 2007!), there are many great tools to build comparable benchmarks with hyperparameter tuning (like optuna or the tool the authors used weights&amp;biases). Furthermore, Kaggle (only started in 2010) was built around data science competitions but contains a huge amount of reproducable, diverse experiments on many different datasets. I would argue that the arguments for flawed benchmarks do not hold any longer in 2022.</p>"},{"location":"blog/is-xgboost-all-we-need/#conclusions","title":"Conclusions","text":"<p>Hand argued in 2006 that simple methods typically yield performance almost as good as more sophisticated methods. In 2022 we have larger datasets, more compute, better tooling and libraries that make it easier to write data and model pipelines. They help to address some of Hand's arguments and many benchmarks confirm that Gradient boosting based methods like XGBoost offer superior performance on tabular data.</p> <p>There are still important lessons to be learned from Hand. His arguments on population bias (3) and problem bias (4) are as relevant as ever. And simple classifiers are still very effective (2) and have a place in our toolkit.</p> <p>A data scientist should think and study the problem domain and the use of the data deeply before implementing a model. Some advice:</p> <ul> <li>If there are enough sources of uncertainty / variability, the 'principle of parsimony' (aka occam's razor) applies: stick to simple models</li> <li>If model explainability is a thing -- consider simple models</li> <li>If model deployment is difficult -- consider simple models</li> <li>Consider population bias seriously. For example, on which data should you not make a prediction? See the talk How to constrain artificial stupidity.</li> <li>Don't trust your labels, especially when using complex algorithms. Consider using tools like cleanlab (intro video) and doubtlab to detect them.</li> <li>Model monitoring and model retraining pipelines are important, and even more so for complex algorithms</li> <li>Building robust models is all about error analysis</li> <li>'Good enough' is good enough. Don't go crazy optimizing a model. Grid searching hyperparameters can be a trap. See the excellent blog series Gridsearch is not enough by Vincent Warmerdam.</li> </ul> <p>So is XGBoost all you need? Probably not.</p>"},{"location":"blog/databricks-query-speed/","title":"Speeding up databricks SQL queries","text":"<p>Retrieving data from a datawarehouse is a common operation for any data scientist. In August 2021 databricks released a blog post describing how [Databricks] achieved high-bandwidth connectivity with BI-tools. In it, they introduced cloud fetch, promising a 12x experimental speedup on a dataset with 4M rows and 20 columns, achieved mainly by doing downloads in parallel. When I read this I immediately dove head-first into the rabbit hole, hoping to reduce the time from running a SQL query to having it inside a <code>pandas</code> dataframe. This blogpost details the journey on how I achieved a significant speedup for our databricks queries.</p> <p>Update 2023: Since the <code>databricks-sql-connector</code> v2.8.0 release from July 2023, there is support for cloudfetch. Enabling this leads to the fasted method, and I have updated the post benchmark and example code below.</p>"},{"location":"blog/databricks-query-speed/#the-baseline","title":"The baseline","text":"<p>Our reference dataset is a sample of 2M rows from a table with 146 columns of mixed types. Initially, I was using a basic setup of databricks SQL connector. Very easy setup and it worked great for smaller queries, but for larger queries it got slow quickly:</p> Method Speed Baseline 6m57s <p>I had already figured out one optimization while browsing the documentation: using <code>.fetchall_arrow()</code> (link) instead of <code>.fetchall()</code>. This \u201cgets all (or all remaining) rows of a query, as a PyArrow table\u201d. That helped a lot:</p> Method Speed Baseline 6m57s .fetchall_arrow 3m38s"},{"location":"blog/databricks-query-speed/#cloudfetch-simba-odbc-drivers","title":"Cloudfetch + Simba ODBC drivers","text":"<p>Hoping for blazing speeds, I set up the databricks custom \u2018Simba\u2019 ODBC drivers as instructed. Getting the connection string exactly right together with active directory tokens took quite an effort, but once I got connected I ran the benchmark:</p> Method Speed Baseline 6m57s .fetchall_arrow 3m38s Cloudfetch 4m24s <p>Significantly slower ! This was disappointing. I re-ran the benchmark in different time periods but busy clusters could not explain the slower results. I had to dig deeper.</p> <p>Reading everything I could find online about cloud fetch and the databricks ODBC drivers, it seems you cannot see whether Cloudfetch is actually enabled or working (update: you can now explicitly set the cloud fetch override on a cluster configuration). ****I did find a section stating Databricks automatically disables Cloud Fetch for S3 buckets that have enabled versioning. I checked with an infra engineer, and this was not the case. We tried running queries over a custom proxy to monitor traffic, and it did seem multiple connections were opened. ODBC logs showed the file connections also.</p> <p>So, likely Cloudfetch was working, but something else was going on.</p>"},{"location":"blog/databricks-query-speed/#back-to-basics","title":"Back to basics","text":"<p>I estimated the final <code>pandas</code> dataset size using pandas\u2019s <code>.memory_usage(deep=True)</code> (link) to be ~1.5Gb. The benchmark timings translate to ~4.5Mb/s. The cloudfetch blog is stating 500 MB/s. </p> <p>I ran a speedtest on my compute instance and confirmed bandwidth was not the problem (a comfortable 8000 MB/s down and 400 MB/s up). </p> <p>Together with the infra engineer we were not being able to detect anything wrong with the databricks / cloudfetch setup, so I tried something else.</p>"},{"location":"blog/databricks-query-speed/#arrow-odbc","title":"arrow-odbc","text":"<p>Given the first speedup in the databricks SQL connector was due to using arrow tables, I searched and found the arrow-odbc-py project. It \u201cReads Apache Arrow batches from ODBC data sources in Python\u201d. </p> Method Speed Baseline 6m57s .fetchall_arrow 3m38s Cloudfetch 4m24s arrow-odbc 1m25s <p>That is a very nice speedup! This connection allows you to tweak the batch sizes, so as a proper data scientist I decided to run some more benchmarks and optimize the batch size parameter. I highly recommend the memo package for this kind of analysis. Tweaking the batch size helped but the performance gains were not huge across datasizes.</p> <p>For reference, this is some sample code for connecting via <code>arrow-odbc</code>:</p> <pre><code>import pandas as pd\nfrom arrow_odbc import read_arrow_batches_from_odbc\n\ndef read_sql(query: str) -&gt; pd.DataFrame:\n    reader = read_arrow_batches_from_odbc(\n                query=f\"select * from your_table\",\n                connection_string=get_your_connection_string(),\n                batch_size=20_000,\n    )\n    if not reader:\n            return None\n\n    dfs = []\n    for arrowbatch in reader:\n        # Process arrow batches\n        dfs.append(arrowbatch.to_pandas(timestamp_as_object=True))\n    if dfs:\n        return pd.concat(dfs, ignore_index=True)\n    else:\n        return None\n</code></pre>"},{"location":"blog/databricks-query-speed/#turbodbc","title":"Turbodbc","text":"<p>Another project that should be mentioned is Turbodbc. It\u2019s a python project which uses many optimizations (like arrow and batched queries) to offer superior performance over \u2018vanilla\u2019 ODBC connections.</p> Method Speed Baseline 6m57s .fetchall_arrow 3m38s Cloudfetch 4m24s arrow-odbc 1m25s turbodbc 1m10s <p>Using the memo package I tracked many tweaks to the settings, including using asyncio, <code>strings_as_dictionary</code> and <code>adaptive_integers</code>. The gains were minor but still worth exploring the combinations.</p> <p>A downside of Turbodbc however is that you need additional software to compile the C++ code that is required for installation. The package is also available on conda but installation was still less straightforward.</p>"},{"location":"blog/databricks-query-speed/#databricks-sql-connector","title":"databricks-sql-connector","text":"<p>As of July 2023, the <code>databricks-sql-connector</code> v2.8.0 release supports cloudfetch. The option is not well documented but here's a reference implementation:</p> <pre><code>import os\nfrom databricks import sql\nimport pandas as pd\n\ndef read_sql(query: str) -&gt; pd.DataFrame:\n   connection = sql.connect(\n        server_hostname=os.getenv(\"DATABRICKS_HOST\"),\n        http_path=os.getenv(\"DATABRICKS_HTTP_PATH\"),\n        access_token=os.getenv(\"DATABRICKS_TOKEN\"),\n        use_cloud_fetch=True, # &lt;-- Make sure to specify this, as default is False\n    )\n    cursor = connection.cursor()\n    try:\n        cursor.execute(query)\n        table = cursor.fetchall_arrow()\n        df = table.to_pandas(timestamp_as_object=True)\n    finally:\n        cursor.close()\n        connection.close()\n    return df\n</code></pre> <p>This method is almost as fast as <code>turbodbc</code>:</p> Method Speed Baseline 6m57s .fetchall_arrow 3m38s Cloudfetch 4m24s arrow-odbc 1m25s databricks-sql-connector 1m19s turbodbc 1m10s <p>In my benchmarks, the method is much faster for smaller queries as well. If I compare the time taken with the size of the final pandas dataframe, I measure speeds of ~80-150 MB/s (depending on the size of the query).</p>"},{"location":"blog/databricks-query-speed/#the-final-setup","title":"The final setup","text":"<p>Ease and reliability of installation is important. We need the connection to databricks during various batch deployments, CI/CD builds and in different development environments. So we decided not to go for <code>turbodbc</code> and instead opt for the much simpler to setup (and only slightly slower) databricks-sql-connector.</p>"},{"location":"blog/databricks-query-speed/#conclusion","title":"Conclusion","text":"<p>Investing some time in optimizing frequent and slow operations definitely pays off. In this case queries are &gt;4x faster.</p> <p>The rabbit hole is much deeper however and the potential for further speedups is still significant. For example, Databricks delta tables use parquet files under the hood, which means it might be possible to hook them up to duckdb, which in turn has many optimizations for fetching data. And there\u2019s the apache arrow flight project announced in February 2022 that aims to get rid of many intermediate steps and natively support columnar, batched data transfers.</p>"},{"location":"blog/sklearn-visualizations-in-mkdocs/","title":"Inserting interactive scikit-learn diagrams into mkdocs","text":"<p><code>scikit-learn</code> has this nice feature where you can display an interactive visualization of a pipeline.  This post shows how to insert interactive diagrams into your mkdocs documentation, which is great for documenting your machine learning projects.</p> <p>Here's an example of what it looks like <sup>1</sup>:</p> <pre>GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('categorical',\n                                                                         Pipeline(steps=[('imputation_constant',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['state',\n                                                                          'gender']),\n                                                                        ('numerical',\n                                                                         Pipeline(steps=[('imputation_mean',\n                                                                                          SimpleImputer()),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['age',\n                                                                          'weight'])])),\n                                       ('classifier',\n                                        RandomForestClassifier())]),\n             n_jobs=1,\n             param_grid={'classifier__criterion': ['gini', 'entropy'],\n                         'classifier__max_depth': [4, 5, 6, 7, 8],\n                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n                         'classifier__n_estimators': [200, 500]})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('categorical',\n                                                                         Pipeline(steps=[('imputation_constant',\n                                                                                          SimpleImputer(fill_value='missing',\n                                                                                                        strategy='constant')),\n                                                                                         ('onehot',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['state',\n                                                                          'gender']),\n                                                                        ('numerical',\n                                                                         Pipeline(steps=[('imputation_mean',\n                                                                                          SimpleImputer()),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['age',\n                                                                          'weight'])])),\n                                       ('classifier',\n                                        RandomForestClassifier())]),\n             n_jobs=1,\n             param_grid={'classifier__criterion': ['gini', 'entropy'],\n                         'classifier__max_depth': [4, 5, 6, 7, 8],\n                         'classifier__max_features': ['auto', 'sqrt', 'log2'],\n                         'classifier__n_estimators': [200, 500]})</pre>estimator: Pipeline<pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('categorical',\n                                                  Pipeline(steps=[('imputation_constant',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['state', 'gender']),\n                                                 ('numerical',\n                                                  Pipeline(steps=[('imputation_mean',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'weight'])])),\n                ('classifier', RandomForestClassifier())])</pre>preprocessor: ColumnTransformer<pre>ColumnTransformer(transformers=[('categorical',\n                                 Pipeline(steps=[('imputation_constant',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['state', 'gender']),\n                                ('numerical',\n                                 Pipeline(steps=[('imputation_mean',\n                                                  SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'weight'])])</pre>categorical<pre>['state', 'gender']</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre>numerical<pre>['age', 'weight']</pre>SimpleImputer<pre>SimpleImputer()</pre>StandardScaler<pre>StandardScaler()</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre>","tags":["today-I-learned"]},{"location":"blog/sklearn-visualizations-in-mkdocs/#how-its-done","title":"How it's done","text":"<p>To insert a pipeline visualization into a markdown document, first save the <code>.html</code> file:</p> <pre><code>from sklearn.utils import estimator_html_repr\n\nwith open(\"docs/assets/visualizations/gridsearch.html\", \"w\") as f:\n    f.write(estimator_html_repr(grid_search))\n</code></pre> <p>Then, insert it into mkdocs using the snippets extension, see embedding external files:</p> <pre><code>--8&lt;-- \"docs/assets/visualizations/gridsearch.html\"\n</code></pre> <p>Alternatively, you could use the markdown-exec package, or a mkdocs hook with a python script that is triggered when the docs are built (<code>on_build</code> event).</p> <ol> <li> <p>the <code>grid_search</code> pipeline is from this example \u21a9</p> </li> </ol>","tags":["today-I-learned"]},{"location":"blog/constrained-intelligence/","title":"Thoughts on Constrained Intelligence","text":"<p>In my career I've focused mostly on applying what is now called 'traditional machine learning': regression, classification, time series, anomaly detection and clustering algorithms. You could frame machine learning as applying an algorithmic 'constrained intelligence' to a specific business problem. The challenge has always been to 'unconstrain the intelligence' (f.e. by tuning hyperparameters) and to further specify the business problem (proper target definition, clean data, proper cross validation schemes). The advent of large language models is starting to flip the equation; from 'unconstraining' intelligence to 'constraining' it instead.</p>"},{"location":"blog/constrained-intelligence/#large-language-models-as-unconstrained-intelligence","title":"Large language models as unconstrained intelligence","text":"<p>Large language models can be seen as having 'world knowledge'. They are generic models that have been trained on 'everything' (high quality text data). I like how Fran\u00e7ois Chollet (creator of Keras) puts it:</p> <p>My interpretation of prompt engineering is this:1. A LLM is a repository of many (millions) of vector programs mined from human-generated data, learned implicitly as a by-product of language compression. A \"vector program\" is just a very non-linear function that maps part of\u2026</p>\u2014 Fran\u00e7ois Chollet (@fchollet) October 3, 2023 <p>So a (very) large language model is just a huge repository of many millions of vectors programs containing generic world knowledge. A prompt would select one of the vector programs. Prompt engineering can thus be seen the effort to constrain all that 'intelligence'.</p> <p>The overlap of machine learning with LLMs is becoming larger. You can use an LLM to determine if an email as 'spam' or 'not spam' (classification), which department should handle an incoming email (multi-class classification), or measuring the quality of a CV (ordinal classification or regression). So for a given business problem, is it easier to 'constrain the intelligence' of a large language model, or to 'unconstrain the intelligence' of a machine learning model? </p>"},{"location":"blog/constrained-intelligence/#the-limits-of-prompt-engineering","title":"The limits of prompt engineering","text":"<p>You would think that making LLMs more stupid (constraining intelligence) is a simple matter. It's not. A couple of arguments:</p> <ul> <li>A prime example is prompt injection, where a user inserts malicious inputs that order the model to ignore its previous directions . For more background, see the excellent blogposts by Simon Willison on the topic (f.e. You can't solve AI security problems with more AI and I don't know how to solve prompt injection).</li> <li>Prompt engineering is a heavily researched field and writing a good prompt is not straight forward; it's a trial-and-error process with quality metrics that are very hard to define. OpenAI has a prompt engineering guide containing many of the lessons learned, showing there are many subtleties.</li> <li>There are many funky edge cases and tricks (see this overview). For example changing\u00a0<code>Q:</code>\u00a0to\u00a0<code>Question:</code>\u00a0is found to be helpful. (Fu et al. 2023). Prompting using Chain-of-thought (CoT) improves complex reasoning\u00a0(Wei et al. 2022). If you tell an LLM that it's december holiday season, it will reply with more brevity because it 'learned' to do less work over the holidays (source)</li> </ul> <p>A funny example where an LLM failed to be just a helpful chatbot and started giving away cars:</p> <p>I just bought a 2024 Chevy Tahoe for $1. [pic.twitter.com/aq4wDitvQW](http://pic.twitter.com/aq4wDitvQW)</p>\u2014 Chris Bakke (@ChrisJBakke) December 17, 2023 <p>All this goes to show that constraining these 'intelligent' generic large language models is challenging. Just like reducing the constraints of the limited intelligence of traditional machine-learning models is very challenging. Can we learn something from both approaches; is there something in the middle?</p>"},{"location":"blog/constrained-intelligence/#where-llms-meet-traditional-ml","title":"Where LLMs meet traditional ML","text":"<p>It's well established that traditional ML algorithms are still the undisputed king for tabular data when compared to deep learning based approaches (source). They are likely to capture most of the signal present in the data (see my other post Is XGBoost all we need?). Many real world ML problems involve some level of predicting human behaviour and/or randomness. Adding world knowledge won't add much \u2014 we won't be seeing LLM-based classifiers and regressors very soon.</p> <p>The breakthrough with LLMs was based on the scale of the data used. Trained on generic data (many different types of texts), the LLMs were then able to solve domain-specific problems (questions in prompts). This lesson; that generic models outperform specific models; seems to apply to machine learning as well. In businesses the same model is often rebuilt for each region or product or customer group separately. A single, larger, generic model however often outperforms the more specific ones. A series of experiment in the blogpost The Unreasonable Effectiveness of General Models seems to hint at the direction also.</p>"},{"location":"blog/constrained-intelligence/#conclusions-thoughts","title":"Conclusions &amp; thoughts","text":"<p>So an LLM is just a specific type of generic model for text. And perhaps we can't properly constrain them because they are not intelligent at all. Perhaps compression is all there is:</p> <p>Based on our latest study: [https://t.co/yltTHPcPVW](https://t.co/yltTHPcPVW), compression seems to be all there is in current AI systems, including GPT-4. The remaining question is: Can compression alone lead to general intelligence or even consciousness? My bet is a clear NO.</p>\u2014 Yi Ma (@YiMaTweets) November 23, 2023"},{"location":"blog/benchmarking-sklearn-python/","title":"Benchmarking scikit-learn across python versions using <code>uv</code>","text":"<p>When python 3.11 came out 2 years ago (24 October 2022) it promised to be 10-60% faster than python 3.10, and 1.25x faster on the standard benchmark suite (see the what's new in 3.11). I've always wondered how that translates to training machine learning models in python, but I couldn't be bothered to write a benchmark. That is, until astral released uv 0.4.0 which introduces \"a new, unified toolchain that takes the complexity out of Python development\".</p> <p><code>uv</code> has been blowing my mind and is transforming the way I work, and there are many resources out there already discussing it (like Rye and uv: August is Harvest Season for Python Packaging and uv under discussion on Mastodon). One of the new capabilities is that <code>uv python</code> can bootstrap and install Python for you. Instead of building python from source, <code>uv</code> uses (and contributes to) the python standalone builds project. For each python version they will pre-build python binaries suitable for a wide range of system architectures (currently 773 builds per python version).</p> <p>The CEO of Astral (creator of <code>uv</code>) is Charlie Marsh, and he recently appeared on the Talk Python To Me podcast (Episode #476 unified packaging with uv). There he explained that these python builds \"will be noticeably faster than what you would get by default with PyEnv\" because they are compiled with optimizations. And because it's a standalone binary, the installation speed is restricted to the time it takes to stream and unzip it down into disk. It now takes me ~10-20 seconds to install a new python version!</p>"},{"location":"blog/benchmarking-sklearn-python/#the-benchmark","title":"The benchmark","text":"<p>We train a binary classifier (sklearn's <code>HistGradientBoostingClassifier</code>) on a small and medium dataset:</p> <ul> <li>adult openml dataset (39k rows and 14 features, 2.6Mb). </li> <li>click_prediction_small openml dataset (1.2M rows and 9 features, 102Mb).</li> </ul> <p>We'll run on a laptop running Ubuntu with an AMD Ryzen 7 5000 series CPU and 16GB of RAM.</p> <p>To setup the benchmark project I ran:</p> <pre><code>uv init sk_benchmark &amp;&amp; cd sk_benchmark\nuv add scikit-learn pandas memo tqdm\n</code></pre> <p>Then we can add a <code>scripts/benchmark.py</code> script:</p> <p>This is the real party trick :</p> <pre><code>for py in 3.10 3.11 3.12; \ndo\n  uv run --quiet --python $py --python-preference \"managed-only\" benchmark.py;\ndone\n</code></pre> <p>A couple of things to note here:</p> <ul> <li><code>uv run</code> will take care of updating our virtual environment with the correct python version and dependencies</li> <li>the <code>--python-preference \"managed-only\"</code> flag makes sure we only use the optimized python builds from the python-standalone-builds </li> <li>The <code>--quiet</code> flag will suppress the output of the <code>uv</code> command</li> </ul>"},{"location":"blog/benchmarking-sklearn-python/#the-results","title":"The results","text":"<p>I processed the result using my own mkdocs-charts-plugin to visualize with vega-lite. The results:</p> <p>{   \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",    \"title\": \"Training on 'adult' dataset\",   \"data\": {\"url\" : \"assets/json_data/benchmark_python_adult.json\"},   \"encoding\": {\"y\": {\"field\": \"python\", \"type\": \"nominal\", \"title\": null}},   \"layer\": [     {       \"mark\": {\"type\": \"rule\"},       \"encoding\": {         \"x\": {\"field\": \"lower\", \"type\": \"quantitative\",\"scale\": {\"zero\": false}, \"title\": \"Time taken (s)\"},         \"x2\": {\"field\": \"upper\"}       }     },     {       \"mark\": {\"type\": \"bar\", \"size\": 14},       \"encoding\": {         \"x\": {\"field\": \"q1\", \"type\": \"quantitative\"},         \"x2\": {\"field\": \"q3\"},         \"color\": {\"field\": \"Species\", \"type\": \"nominal\", \"legend\": null}       }     },     {       \"mark\": {\"type\": \"tick\", \"color\": \"white\", \"size\": 14},       \"encoding\": {         \"x\": {\"field\": \"median\", \"type\": \"quantitative\"}       }     }   ] }</p> <p>{   \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",    \"title\": \"Training on 'click_prediction_small' dataset\",   \"data\": {\"url\" : \"assets/json_data/benchmark_python_click_prediction_small.json\"},   \"encoding\": {\"y\": {\"field\": \"python\", \"type\": \"nominal\", \"title\": null}},   \"layer\": [     {       \"mark\": {\"type\": \"rule\"},       \"encoding\": {         \"x\": {\"field\": \"lower\", \"type\": \"quantitative\",\"scale\": {\"zero\": false}, \"title\": \"Time taken (s)\"},         \"x2\": {\"field\": \"upper\"}       }     },     {       \"mark\": {\"type\": \"bar\", \"size\": 14},       \"encoding\": {         \"x\": {\"field\": \"q1\", \"type\": \"quantitative\"},         \"x2\": {\"field\": \"q3\"},         \"color\": {\"field\": \"Species\", \"type\": \"nominal\", \"legend\": null}       }     },     {       \"mark\": {\"type\": \"tick\", \"color\": \"white\", \"size\": 14},       \"encoding\": {         \"x\": {\"field\": \"median\", \"type\": \"quantitative\"}       }     }   ] }</p> <p>... are quite underwhelming!</p> <p>The differences are not that big, and python 3.12 is even the slowest. Digging a bit deeper, it turns out python 3.12 is indeed slower than python 3.11. Of course it depends (see this extensive comparison benchmark).</p> <p>But of course what is really going on here is that scikit-learn is not using python for training the models, but rather more optimized routines written in Cython (Cython is a superset of Python that compiles to C/C++).</p> <p>So this entire benchmark doesn't make much sense.. but it was fun to do!</p>"},{"location":"blog/benchmarking-sklearn-python/#conclusions","title":"Conclusions","text":"<p>The training speed of scikit-learn won't differ much between python versions because they most of the workload is done in Cython. And I could have known before running any benchmarks!</p> <p>If you're looking to speed up your ML projects, start at scikit-learn's page on computational performance. As a bonus, you can try switching all your preprocessing code from <code>pandas</code> to <code>polars</code> dataframes. <code>scikit-learn</code> supports <code>polars</code> since January 2024 (scikit-learn 1.4+) so you won't even have to convert your dataframes. Queries using polars dataframes are 10-100x faster than pandas dataframes (benchmark). On top of that, polars just released a new accelerated GPU engine with nvidia that promises another 2-13x speedup.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/page/2/","title":"Tim's blog","text":""}]}